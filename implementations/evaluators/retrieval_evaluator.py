import math
from typing import List, Dict, Any, Set


class RetrieverEvaluator:
    """ê²€ìƒ‰ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° class (í‰ê°€ì§€í‘œ ìˆ˜ì •!!)"""

    def __init__(self, ground_truth_size: int = 5):
        self.gt_size = ground_truth_size

    def evaluate_query(
        self,
        retrieved_rec_idxs: List[str],
        ground_truth_rec_idxs: List[str],
        search_time: float = None,
    ) -> Dict[str, float]:
        """
        ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ ëª¨ë“  ì§€í‘œ ê³„ì‚°

        Args:
            retrieved_rec_idxs: ê²€ìƒ‰ëœ rec_idx ë¦¬ìŠ¤íŠ¸ (ìˆœì„œëŒ€ë¡œ, ìµœì†Œ 20ê°œ)
            ground_truth_rec_idxs : ì •ë‹µ rec_idx ë¦¬ìŠ¤íŠ¸ (5ê°œ)
            search_time: ê²€ìƒ‰ ì‹œê°„ (ì´ˆ, ì„ íƒ)
        Returns:
            {'ndcg@10': 0.72, 'recall@20': 0.8, 'mrr@10': 0.5, 'search_time': 0.421, ...}
        """
        metrics = {
            "ndcg@10": self.calculate_ndcg_at_k(
                retrieved_rec_idxs, ground_truth_rec_idxs, k=10
            ),
            "recall@20": self.calculate_recall_at_k(
                retrieved_rec_idxs, ground_truth_rec_idxs, k=20
            ),
            "mrr@10": self.calculate_mrr_at_k(
                retrieved_rec_idxs, ground_truth_rec_idxs, k=10
            ),
            "precision@3": self.calculate_precision_at_k(
                retrieved_rec_idxs, ground_truth_rec_idxs, k=3
            ),
            "precision@5": self.calculate_precision_at_k(
                retrieved_rec_idxs, ground_truth_rec_idxs, k=5
            ),
        }
        # ì¶”ê°€ ì •ë³´
        metrics["hits@20"] = len(
            set(retrieved_rec_idxs[:20]) & set(ground_truth_rec_idxs)
        )
        metrics["total_gt"] = len(ground_truth_rec_idxs)

        # ê²€ìƒ‰ ì‹œê°„ ì¶”ê°€ (ì œê³µëœ ê²½ìš°)
        if search_time is not None:
            metrics["search_time"] = search_time

        return metrics

    def calculate_ndcg_at_k(
        self, retrieved_ids: List[str], ground_truth_ids: List[str], k=10
    ) -> float:
        """

        NDCG@10 : ìˆœìœ„ í’ˆì§ˆ ì¸¡ì •
        - DCG: ê° ìœ„ì¹˜ì˜ ê´€ë ¨ì„± / log2(rank+1)
        - IDCG: ìµœì  ìˆœìœ„ì˜ DCG
        - NDCG: DCG / IDCG (0-1 ì •ê·œí™”)

        Args:
            retrieved_ids: ê²€ìƒ‰ëœ rec_idx ìˆœìœ„
            ground_truth_ids: ì •ë‹µ
            k: ìƒìœ„ kê°œ í‰ê°€
        Return:
            0.0~ 1.0 (1.0ì´ ê°€ì¥ ì¢‹ìŒ)
        """
        gt_set = set(ground_truth_ids)

        # DCG ê³„ì‚°
        # DCG ê³„ì‚°
        dcg = 0.0
        for i, rec_idx in enumerate(retrieved_ids[:k], start=1):
            relevance = 1.0 if rec_idx in gt_set else 0.0
            dcg += relevance / math.log2(i + 1)
        # IDCG ê³„ì‚° (ì´ìƒì  ìˆœì„œ: ëª¨ë“  ì •ë‹µì´ ë§¨ ì•ì—)
        idcg = 0.0
        for i in range(1, min(len(ground_truth_ids), k) + 1):
            idcg += 1.0 / math.log2(i + 1)
        # NDCG ì •ê·œí™”
        if idcg == 0:
            return 0.0
        return dcg / idcg

    def calculate_recall_at_k(
        self, retrieved_ids: List[str], ground_truth_ids: List[str], k=20
    ) -> float:
        """
        Recall@20 : ì •ë‹µ ì¬í˜„ìœ¨
        - ìˆ˜ì‹: (ìƒìœ„ kê°œì— í¬í•¨ëœ ì •ë‹µ ê°œìˆ˜) / (ì „ì²´ ì •ë‹µ ê°œìˆ˜)

        - ë¶„ëª¨ : í•­ìƒ gt ì˜ ê°œìˆ˜ (5ê°œ)
        - ë¶„ì : ìƒìœ„ kê°œì— í¬í•¨ëœ ì •ë‹µ ê°œìˆ˜
        - recall@20 = hits / 5

        Args:
            retreived_ids: ê²€ìƒ‰ëœ rec_idx ê²°ê³¼
            ground_truth_ids: ì •ë‹µ rec_idx ë¦¬ìŠ¤íŠ¸
            k : ìƒìœ„ kê°œ vudrk
        Returns:
            0.0~ 1.0 (1.0ì´ ê°€ì¥ ì¢‹ìŒ)
        Example:
            GT = [A, B, C, D, E] (5ê°œ)
            Retrieved@20 = [X, A, Y, B, Z, ..., C, ...]
            Hits = 3 (A, B, C)
            Recall@20 = 3 / 5 = 0.6
        """
        if len(ground_truth_ids) == 0:
            return 0.0
        retrieved_set = set(retrieved_ids[:k])
        gt_set = set(ground_truth_ids)

        hits = len(retrieved_set & gt_set)

        # ë¶„ëª¨: ì „ì²´ ì •ë‹µ ê°œìˆ˜ 5ê°œë¡œ ê³ ì •
        return hits / len(ground_truth_ids)

    def calculate_mrr_at_k(
        self, retrieved_ids: List[str], ground_truth_ids: List[str], k=10
    ) -> float:
        """
        MRR@k : ì²« ì •ë‹µ ìˆœìœ„ì˜ ì—­ìˆ˜
        ìˆ˜ì‹ :
            - MRR = 1/ rank (ì²«ë²ˆì§¸ ì •ë‹µ ìœ„ì¹˜)
        Args:
            retrieved_ids: ê²€ìƒ‰ ê²°ê³¼ (ìˆœì„œ ì¤‘ìš”)
            ground_truth_ids: ì •ë‹µ rec_idx ë¦¬ìŠ¤íŠ¸
            K: ìƒìœ„ kê°œì—ì„œë§Œ ì°¾ê¸°
        Returns:
            0.0~ 1.0 (1.0ì´ ê°€ì¥ ì¢‹ìŒ)
        Example:
            GT = [A, B, C, D, E] (5ê°œ)
            Retrieved@10 = [X, A, Y, B, Z, ..., C, ...]
            First relevant rank = 2 (A)
            MRR@10 = 1/2 = 0.5
        """
        gt_set = set(ground_truth_ids)

        for rank, rec_idx in enumerate(retrieved_ids[:k], start=1):
            if rec_idx in gt_set:
                return 1.0 / rank
        # ìƒìœ„ kê°œ ì•ˆì— ì •ë‹µ ì—†ìŒ
        return 0.0

    def calculate_precision_at_k(
        self, retrieved_ids: List[str], ground_truth_ids: List[str], k: int
    ) -> float:
        """
        precision@k : ìƒìœ„ Kê°œì¤‘ ì •ë‹µì˜ ë¹„ìœ¨
        ìˆ˜ì‹:
        - precision@k = (ìƒìœ„ kê°œì— í¬í•¨ëœ ì •ë‹µ ê°œìˆ˜) / k
        Args:
            retrieved_ids: ê²€ìƒ‰ëœ rec_idx ê²°ê³¼
            ground_truth_ids: ì •ë‹µ rec_idx ë¦¬ìŠ¤íŠ¸
            k: ìƒìœ„ kê°œ vudrk
        Returns:
            0.0~ 1.0 (1.0ì´ ê°€ì¥ ì¢‹ìŒ)
        Example:
            GT = [A, B, C, D, E] (5ê°œ)
            Retrieved@3 = [A, X, B]
            Hits = 2 (A, B)
            Precision@3 = 2 / 3 = 0.67
        """
        if k == 0:
            return 0.0
        retrieved_set = set(retrieved_ids[:k])
        gt_set = set(ground_truth_ids)

        hits = len(retrieved_set & gt_set)

        return hits / k  # k 3,5ê³ ì •

    def evaluate_all_queries(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ì¿¼ë¦¬ì˜ í‰ê°€ ê²°ê³¼ ì§‘ê³„
        Args: results:[{
            "query_id": "437" ,
            "retrieved_rec_idxs": ["123", "456", "789"],
            "ground_truth_rec_idxs": ["123", "456", "789"]
            "metrics":{...}
        }, ...]
        Returns:
            {
                "total_queries": 115,
                "average_metrics": {
                    "ndcg@10": 0.72,
                    ...
                },
                per_query_metrics: {... }
            }
        """
        total_queries = len(results)

        if total_queries == 0:
            return {"error": "No queries to evaluate"}

        # ê° ì§€í‘œë³„ í•©ì‚°
        metric_sums = {
            "ndcg@10": 0.0,
            "recall@20": 0.0,
            "mrr@10": 0.0,
            "precision@3": 0.0,
            "precision@5": 0.0,
        }
        per_query_metrics = {}

        for result in results:
            query_id = result["query_id"]
            metrics = result.get("metrics", {})

            per_query_metrics[query_id] = metrics

            for metric_name in metric_sums.keys():
                metric_sums[metric_name] += metrics.get(metric_name, 0.0)

        # í‰ê·  ê³„ì‚°
        average_metrics = {
            metric_name: total / total_queries
            for metric_name, total in metric_sums.items()
        }

        return {
            "total_queries": total_queries,
            "average_metrics": average_metrics,
            "per_query_metrics": per_query_metrics,
        }


# ========================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ========================================


def print_evaluation_summary(summary: Dict[str, Any]):
    """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
    print(f"{'='*60}")
    print(f"ì´ ì¿¼ë¦¬: {summary['total_queries']}ê°œ\n")

    avg_metrics = summary["average_metrics"]

    print("í‰ê·  ì§€í‘œ:")
    print(f"  NDCG@10:      {avg_metrics['ndcg@10']:.4f}")
    print(f"  Recall@20:    {avg_metrics['recall@20']:.4f}")
    print(f"  MRR@10:       {avg_metrics['mrr@10']:.4f}")
    print(f"  Precision@3:  {avg_metrics['precision@3']:.4f}")
    print(f"  Precision@5:  {avg_metrics['precision@5']:.4f}")
    print(f"{'='*60}\n")


# ========================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ (ì‹¤ì œ evaluation_queries.jsonl í˜•ì‹)
# ========================================

if __name__ == "__main__":
    print("=" * 70)
    print("ğŸ§ª RetrieverEvaluator í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ë°ì´í„° í˜•ì‹)")
    print("=" * 70)

    evaluator = RetrieverEvaluator(ground_truth_size=5)

    # ğŸ“Š ì‹¤ì œ evaluation_queries.jsonl í˜•ì‹ ì‹œë®¬ë ˆì´ì…˜
    query_data = {
        "query_id": "437",
        "query_text": "ì „ê³µ: ìƒëª…ê³µí•™\nê´€ì‹¬ ì§ë¬´: ìƒëª…ê³µí•™ ì—°êµ¬ì›...",
        "ground_truth": [
            {"rec_idx": "50436465", "job_title": "[í•œêµ­ì½œë§ˆ] ì—°êµ¬ì „ëµ", "url": "..."},
            {
                "rec_idx": "50436592",
                "job_title": "[í•œêµ­ì½œë§ˆ] ì»´í”Œë¼ì´ì–¸ìŠ¤",
                "url": "...",
            },
            {"rec_idx": "50436291", "job_title": "[í•œêµ­ì½œë§ˆ] ë§ˆì¼€íŒ…", "url": "..."},
            {"rec_idx": "50436627", "job_title": "[í•œêµ­ì½œë§ˆ] ìƒì‚°ê´€ë¦¬", "url": "..."},
            {"rec_idx": "50436344", "job_title": "[í•œêµ­ì½œë§ˆ] ì†Œì¬ê°œë°œ", "url": "..."},
        ],
    }

    # âœ… Pipelineì—ì„œ í•˜ëŠ” ê²ƒì²˜ëŸ¼ rec_idx ì¶”ì¶œ
    gt_rec_idxs = [str(gt["rec_idx"]) for gt in query_data["ground_truth"]]
    print(f"\nğŸ“Œ Query ID: {query_data['query_id']}")
    print(f"ğŸ“Œ Ground Truth (GT): {len(gt_rec_idxs)}ê°œ")
    print(f"   GT rec_idx: {gt_rec_idxs}\n")

    # ========================================
    # ì‹œë‚˜ë¦¬ì˜¤ 1: ì™„ë²½í•œ ê²€ìƒ‰ (ëª¨ë“  ì •ë‹µì´ ìƒìœ„ 5ê°œì—)
    # ========================================
    print("\n" + "=" * 70)
    print("âœ… ì‹œë‚˜ë¦¬ì˜¤ 1: ì™„ë²½í•œ ê²€ìƒ‰ (ìƒìœ„ 5ê°œì— ëª¨ë“  GT í¬í•¨)")
    print("=" * 70)

    perfect_retrieval = gt_rec_idxs + [f"9999{i}" for i in range(15)]  # 20ê°œ
    print(f"ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 10ê°œ): {perfect_retrieval[:10]}")

    metrics1 = evaluator.evaluate_query(perfect_retrieval, gt_rec_idxs)
    print("\nğŸ“Š í‰ê°€ ì§€í‘œ:")
    for metric, value in metrics1.items():
        print(f"  {metric:15s}: {value}")

    # ========================================
    # ì‹œë‚˜ë¦¬ì˜¤ 2: ì¼ë¶€ ì •ë‹µë§Œ ê²€ìƒ‰ (3ê°œë§Œ ìƒìœ„ 10ê°œì—)
    # ========================================
    print("\n" + "=" * 70)
    print("âš ï¸  ì‹œë‚˜ë¦¬ì˜¤ 2: ì¼ë¶€ ì •ë‹µë§Œ ê²€ìƒ‰ (ìƒìœ„ 10ê°œì— 3ê°œë§Œ)")
    print("=" * 70)

    # 1ìœ„: ì˜¤ë‹µ, 2ìœ„: GT[0], 4ìœ„: GT[1], 8ìœ„: GT[2], ë‚˜ë¨¸ì§€ëŠ” í•˜ìœ„
    partial_retrieval = [
        "88888888",  # 1ìœ„: ì˜¤ë‹µ
        gt_rec_idxs[0],  # 2ìœ„: 50436465 âœ…
        "99999999",  # 3ìœ„: ì˜¤ë‹µ
        gt_rec_idxs[1],  # 4ìœ„: 50436592 âœ…
        "77777777",  # 5ìœ„: ì˜¤ë‹µ
        "66666666",  # 6ìœ„: ì˜¤ë‹µ
        "55555555",  # 7ìœ„: ì˜¤ë‹µ
        gt_rec_idxs[2],  # 8ìœ„: 50436291 âœ…
        "44444444",  # 9ìœ„: ì˜¤ë‹µ
        "33333333",  # 10ìœ„: ì˜¤ë‹µ
    ] + [
        f"1111{i}" for i in range(10)
    ]  # 11-20ìœ„: ì˜¤ë‹µ

    print(f"ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 10ê°œ):")
    for i, rec_idx in enumerate(partial_retrieval[:10], start=1):
        is_gt = "âœ… GT" if rec_idx in gt_rec_idxs else "âŒ"
        print(f"  {i:2d}ìœ„: {rec_idx} {is_gt}")

    metrics2 = evaluator.evaluate_query(partial_retrieval, gt_rec_idxs)
    print("\nğŸ“Š í‰ê°€ ì§€í‘œ:")
    for metric, value in metrics2.items():
        print(f"  {metric:15s}: {value}")

    # ========================================
    # ì‹œë‚˜ë¦¬ì˜¤ 3: ì •ë‹µì´ í•˜ìœ„ì— (15ìœ„ ì´í›„)
    # ========================================
    print("\n" + "=" * 70)
    print("âŒ ì‹œë‚˜ë¦¬ì˜¤ 3: ì •ë‹µì´ í•˜ìœ„ì— (15-20ìœ„)")
    print("=" * 70)

    poor_retrieval = [
        f"9999{i}" for i in range(15)
    ] + gt_rec_idxs  # 1-15ìœ„ ì˜¤ë‹µ, 16-20ìœ„ ì •ë‹µ
    print(f"ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 10ê°œ): {poor_retrieval[:10]}")
    print(f"ê²€ìƒ‰ ê²°ê³¼ (16-20ìœ„): {poor_retrieval[15:20]}")

    metrics3 = evaluator.evaluate_query(poor_retrieval, gt_rec_idxs)
    print("\nğŸ“Š í‰ê°€ ì§€í‘œ:")
    for metric, value in metrics3.items():
        print(f"  {metric:15s}: {value}")

    # ========================================
    # ì „ì²´ ê²°ê³¼ ë¹„êµ
    # ========================================
    print("\n" + "=" * 70)
    print("ğŸ“Š ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ ë¹„êµ")
    print("=" * 70)

    results = [
        {"query_id": "ì‹œë‚˜ë¦¬ì˜¤1_ì™„ë²½", "metrics": metrics1},
        {"query_id": "ì‹œë‚˜ë¦¬ì˜¤2_ì¼ë¶€", "metrics": metrics2},
        {"query_id": "ì‹œë‚˜ë¦¬ì˜¤3_í•˜ìœ„", "metrics": metrics3},
    ]

    summary = evaluator.evaluate_all_queries(results)
    print_evaluation_summary(summary)

    print("\n" + "=" * 70)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 70)
