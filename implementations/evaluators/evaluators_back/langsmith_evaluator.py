"""
LangSmith ê¸°ë°˜ ê³ í’ˆì§ˆ ì‘ë‹µ í‰ê°€

LLM-as-Judge ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„± í’ˆì§ˆì„ ì •êµí•˜ê²Œ í‰ê°€
"""

import json
import asyncio
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

from langsmith import Client, traceable
from langsmith.evaluation import evaluate
from langchain_openai import ChatOpenAI


@dataclass
class LangSmithEvaluationResult:
    """LangSmith í‰ê°€ ê²°ê³¼"""

    metric_name: str
    score: float
    reasoning: str
    details: Dict[str, Any]


class CareerHYLangSmithEvaluator:
    """Career-HY ë§ì¶¤í˜• LangSmith í‰ê°€ê¸°"""

    def __init__(
        self,
        judge_model: str = "gpt-4o-mini",
        project_name: str = "career-hy-rag-evaluation",
        base_tags: List[str] = None,
    ):
        self.judge_model = judge_model
        self.project_name = project_name
        self.base_tags = base_tags or []
        self.client = Client()
        self.llm = ChatOpenAI(model=judge_model, temperature=0.1)

        # í‰ê°€ ì§€í‘œë“¤
        self.metrics = [
            "recommendation_quality",
            "personalization_score",
            "response_helpfulness",
            "profile_alignment",
        ]

    async def evaluate_batch(
        self, query_results: List[Dict[str, Any]], experiment_name: str
    ) -> List[LangSmithEvaluationResult]:
        """
        ì—¬ëŸ¬ ì¿¼ë¦¬ ê²°ê³¼ì— ëŒ€í•œ LangSmith ë°°ì¹˜ í‰ê°€

        Args:
            query_results: ì¿¼ë¦¬ë³„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            experiment_name: ì‹¤í—˜ëª… (LangSmith ì¶”ì ìš©)

        Returns:
            í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """

        print(f"ğŸ” LangSmith í‰ê°€ ì‹œì‘: {len(query_results)}ê°œ ì¿¼ë¦¬")
        print(f"ğŸ¤– Judge ëª¨ë¸: {self.judge_model}")
        print(f"ğŸ“Š í‰ê°€ ì§€í‘œ: {self.metrics}")

        # ì§ì ‘ í‰ê°€ ì‹¤í–‰
        final_results = []

        for metric_name in self.metrics:
            print(f"\nğŸ“ˆ {metric_name} í‰ê°€ ì¤‘...")

            try:
                scores = []
                reasonings = []

                for i, result in enumerate(query_results):
                    evaluation_result = await self._evaluate_single_query(
                        result, metric_name
                    )
                    scores.append(evaluation_result["score"])
                    reasonings.append(evaluation_result["reasoning"])

                avg_score = sum(scores) / len(scores) if scores else 0.0

                final_results.append(
                    LangSmithEvaluationResult(
                        metric_name=metric_name,
                        score=avg_score,
                        reasoning=f"í‰ê·  ì ìˆ˜: {avg_score:.3f}",
                        details={
                            "individual_scores": scores,
                            "individual_reasonings": reasonings,
                            "total_queries": len(query_results),
                        },
                    )
                )

                print(f"âœ… {metric_name} í‰ê°€ ì™„ë£Œ: {avg_score:.3f}")

            except Exception as e:
                print(f"âŒ {metric_name} í‰ê°€ ì‹¤íŒ¨: {e}")
                final_results.append(
                    LangSmithEvaluationResult(
                        metric_name=metric_name,
                        score=0.0,
                        reasoning=f"í‰ê°€ ì‹¤íŒ¨: {e}",
                        details={"error": str(e)},
                    )
                )

        print(f"\nğŸ‰ LangSmith í‰ê°€ ì™„ë£Œ!")
        for result in final_results:
            print(f"  {result.metric_name}: {result.score:.3f}")

        return final_results

    async def _evaluate_single_query(
        self, query_result: Dict[str, Any], metric_name: str
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ íŠ¹ì • ì§€í‘œ í‰ê°€"""

        try:
            query = query_result.get("query", "")
            user_profile = query_result.get("user_profile", {})
            generated_response = query_result.get("generated_response", {})
            alternative_query = query_result.get("alternative_query", "")

            # ë©”íŠ¸ë¦­ë³„ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±
            if metric_name == "recommendation_quality":
                prompt = self._create_recommendation_quality_prompt(
                    query, user_profile, generated_response, alternative_query
                )
            elif metric_name == "personalization_score":
                prompt = self._create_personalization_prompt(
                    query, user_profile, generated_response, alternative_query
                )
            elif metric_name == "response_helpfulness":
                prompt = self._create_helpfulness_prompt(
                    query, user_profile, generated_response, alternative_query
                )
            elif metric_name == "profile_alignment":
                prompt = self._create_profile_alignment_prompt(
                    user_profile, generated_response
                )
            else:
                raise ValueError(f"Unknown metric: {metric_name}")

            # LLMìœ¼ë¡œ í‰ê°€ ì‹¤í–‰
            response = await self.llm.ainvoke(prompt)
            response_text = response.content

            # ì ìˆ˜ì™€ ì´ìœ  ì¶”ì¶œ
            score = self._extract_score(response_text)
            reasoning = self._extract_reasoning(response_text)

            return {
                "score": score,
                "reasoning": reasoning,
                "full_response": response_text,
            }

        except Exception as e:
            return {
                "score": 0.0,
                "reasoning": f"í‰ê°€ ì‹¤íŒ¨: {e}",
                "full_response": str(e),
            }

    # ì¶”ì²œ í’ˆì§ˆ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±
    def _create_recommendation_quality_prompt(
        self,
        query: str,
        user_profile: Dict,
        generated_response: Dict,
        alternative_query: str = "",
    ) -> str:
        """ì¶”ì²œ í’ˆì§ˆ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        # alternative_queryê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ query ì‚¬ìš©
        display_query = alternative_query if alternative_query else query
        # í‰ê°€ ëŒ€ìƒ : ì¶”ì²œ ê³µê³  ë¦¬ìŠ¤íŠ¸, ì¡°ì–¸ / ì„¤ëª… í…ìŠ¤íŠ¸
        recommended_jobs = json.dumps(
            generated_response.get("recommended_jobs", []), ensure_ascii=False, indent=2
        )
        recommendation_content = generated_response.get("content", "")
        return f"""
ë‹¤ìŒ ì±„ìš©ê³µê³  ì¶”ì²œì˜ í’ˆì§ˆì„ 1-5ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

[ì •ë³´]
ì‚¬ìš©ì ì˜ë„(ìµœì¢… ì§ˆë¬¸): {display_query}

ì‚¬ìš©ì í”„ë¡œí•„: 
- ì „ê³µ: {user_profile.get('major', 'N/A')}
- ê´€ì‹¬ì§ë¬´: {user_profile.get('interest_job', 'N/A')}


[í‰ê°€ ëŒ€ìƒ]
1. ì¶”ì²œ ê³µê³  ë¦¬ìŠ¤íŠ¸: {recommended_jobs}
2. ìƒì„±ëœ ì¡°ì–¸ ë° ì„¤ëª…: {recommendation_content}

í‰ê°€ ê¸°ì¤€:
- ì¶”ì²œëœ ê³µê³ (1)ê°€ 'ì‚¬ìš©ì ì˜ë„' ë° 'í”„ë¡œí•„'ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ 
- ìƒì„±ëœ ì¡°ì–¸(2)ì´ 'ì‚¬ìš©ì ì˜ë„'ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ì„ í•˜ëŠ”ì§€ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.

[ì ìˆ˜ ê¸°ì¤€]
- 5ì ( ë§¤ìš°ìš°ìˆ˜): 5ì  (ë§¤ìš° ìš°ìˆ˜): ì¶”ì²œ ê³µê³ ê°€ 'ì‚¬ìš©ì ì˜ë„' ë° 'í”„ë¡œí•„'ê³¼ ì™„ë²½íˆ ì¼ì¹˜í•©ë‹ˆë‹¤. 'ìƒì„±ëœ ì¡°ì–¸'ì€ ë§¤ìš° êµ¬ì²´ì ì´ê³ , ì¶”ì²œ ì´ìœ ê°€ ëª…í™•í•˜ë©° ì‹¤ì§ˆì ìœ¼ë¡œ ìœ ìš©í•©ë‹ˆë‹¤.
- 4ì  (ìš°ìˆ˜): ëŒ€ë¶€ë¶„ì˜ ê³µê³ ê°€ ì¼ì¹˜í•˜ë©° ì¡°ì–¸ë„ ìœ ìš©í•©ë‹ˆë‹¤.
    **(ì¤‘ìš”!) ë§Œì•½ ì¶”ì²œ ê³µê³ ê°€ ì‚¬ìš©ì ì˜ë„ì™€ ë‹¤ì†Œ ë‹¤ë¥´ë”ë¼ë„, ì‚¬ìš©ì í”„ë¡œí•„ì— ë” ì í•©í•˜ë‹¤ê³  íŒë‹¨í•˜ì—¬ 'ìƒì„±ëœ ì¡°ì–¸'ì—ì„œ ê·¸ ì´ìœ ë¥¼ ëª…í™•íˆ ì„¤ëª…í–ˆë‹¤ë©´ 4-5ì ì„ ë¶€ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
- 3ì  (ë³´í†µ): ì¼ë¶€ ê³µê³ ë§Œ ì¼ì¹˜í•˜ê±°ë‚˜ ì¡°ì–¸ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.
- 2ì  (ë¶€ì¡±):ëŒ€ë¶€ë¶„ì˜ ê³µê³ ê°€ ê´€ë ¨ ì—†ê±°ë‚˜, ì¡°ì–¸ì´ ì§ˆë¬¸ê³¼ ë™ë–¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
- 1ì  (ë¯¸í¡): ì™„ì „íˆ ë¶€ì ì ˆí•œ ì¶”ì²œì…ë‹ˆë‹¤.

ì ìˆ˜ (1-5ì ):
ì´ìœ :
"""

    def _create_personalization_prompt(
        self,
        query: str,
        user_profile: Dict,
        generated_response: Dict,
        alternative_query: str = "",
    ) -> str:
        """ê°œì¸í™” í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        display_query = alternative_query if alternative_query else query
        recommendation_content = generated_response.get("content", "")
        return f"""
ë‹¤ìŒ 'ì¶”ì²œ ì¡°ì–¸'ì´ ì‚¬ìš©ìì—ê²Œ ì–¼ë§ˆë‚˜ 'ê°œì¸í™”'ë˜ì–´ ìˆëŠ”ì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
(ì´ í‰ê°€ëŠ” ì¶”ì²œëœ ê³µê³  ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ, 'ìƒì„±ëœ ì¡°ì–¸' í…ìŠ¤íŠ¸ ìì²´ì— ì§‘ì¤‘í•©ë‹ˆë‹¤.)

[ì •ë³´]
ì‚¬ìš©ì ì˜ë„(ìµœì¢… ì§ˆë¬¸): {display_query}

ì‚¬ìš©ì í”„ë¡œí•„:
- ì „ê³µ: {user_profile.get('major', 'N/A')}
- ê´€ì‹¬ ì§ë¬´: {user_profile.get('interest_job', 'N/A')}

[í‰ê°€ëŒ€ìƒ]
- ìƒì„±ëœ ì¡°ì–¸:
{recommendation_content}

[í‰ê°€ ê¸°ì¤€]
- ì´ ì¡°ì–¸ì´ 'ì‚¬ìš©ì í”„ë¡œí•„'(ì „ê³µ, ê´€ì‹¬ ì§ë¬´)ì˜ ìš”ì†Œë¥¼ ì–¼ë§ˆë‚˜ êµ¬ì²´ì ìœ¼ë¡œ 'ì–¸ê¸‰'í•˜ê³  'ë°˜ì˜'í•˜ì—¬ ë§ì¶¤í˜• ì¡°ì–¸ì„ ì œê³µí•˜ëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.

[ì ìˆ˜ ê¸°ì¤€]
- 5ì  (ë§¤ìš° ìš°ìˆ˜): ì‚¬ìš©ìì˜ ì „ê³µ, ê´€ì‹¬ ì§ë¬´ë¥¼ ëª…í™•íˆ ì–¸ê¸‰í•˜ë©°, ì´ì™€ ì§ì ‘ì ìœ¼ë¡œ ì—°ê²°ëœ êµ¬ì²´ì ì¸ í–‰ë™(ì˜ˆ: 'ê²½ì˜í•™ ì „ê³µì´ì‹œë‹ˆ OOO ê²½í—˜ì„ ê°•ì¡°í•˜ì„¸ìš”')ì„ ì œì•ˆí•©ë‹ˆë‹¤.
- 3ì  (ë³´í†µ): í”„ë¡œí•„ ìš”ì†Œë¥¼ ì–¸ê¸‰í•˜ê¸°ëŠ” í•˜ë‚˜, 'ê´€ë ¨ ê²½í—˜ì„ ìŒ“ìœ¼ì„¸ìš”'ì²˜ëŸ¼ ì¼ë°˜ì ì¸ ìˆ˜ì¤€ì˜ ì¡°ì–¸ì— ê·¸ì¹©ë‹ˆë‹¤.
- 1ì  (ë§¤ìš° ë¯¸í¡): í”„ë¡œí•„ì„ ì „í˜€ ë°˜ì˜í•˜ì§€ ì•Šì€(ì˜ˆ: "ë‹¹ì‹ ì—ê²Œ ë§ëŠ” ê³µê³ ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤") í…œí”Œë¦¿ í˜•íƒœì˜ ì¼ë°˜ì ì¸ ì¡°ì–¸ì…ë‹ˆë‹¤.

ì ìˆ˜ (1-5ì ):
ì´ìœ :
"""

    def _create_helpfulness_prompt(
        self,
        query: str,
        user_profile: Dict,
        generated_response: Dict,
        alternative_query: str = "",
    ) -> str:
        """ë„ì›€ì´ ë˜ëŠ” ì •ë„ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        display_query = alternative_query if alternative_query else query
        recommendation_content = generated_response.get("content", "")
        return f"""
ë‹¤ìŒ 'ì¶”ì²œ ì¡°ì–¸'ì´ ì‚¬ìš©ìì—ê²Œ 'ì‹¤ì§ˆì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ë„ì›€ì´ ë˜ëŠ”ì§€' 1-5ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
(ì´ í‰ê°€ëŠ” ì¶”ì²œëœ ê³µê³  ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ, 'ìƒì„±ëœ ì¡°ì–¸' í…ìŠ¤íŠ¸ ìì²´ì— ì§‘ì¤‘í•©ë‹ˆë‹¤.)

[ì •ë³´]
- ì‚¬ìš©ì ì˜ë„ (ìµœì¢… ì§ˆë¬¸): {display_query}

[í‰ê°€ëŒ€ìƒ]
- ìƒì„±ëœ ì¡°ì–¸:
{recommendation_content}

[í‰ê°€ ê¸°ì¤€]
- ì´ ì¡°ì–¸ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ í”„ë¡œí•„ì„ ë°”íƒ•ìœ¼ë¡œ, ì·¨ì—… ì¤€ë¹„ì— ì‹¤ì§ˆì ì¸ ë„ì›€ì´ ë˜ëŠ” 'êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ(actionable)' ì •ë³´ë¥¼ ì œê³µí•˜ëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.

[ì ìˆ˜ ê¸°ì¤€]
- 5ì  (ë§¤ìš° ìš°ìˆ˜): 'ì¸í„´ì‹­ ì§€ì›', 'í¬íŠ¸í´ë¦¬ì˜¤ì— OOO í”„ë¡œì íŠ¸ ì¶”ê°€', 'ê´€ë ¨ ìê²©ì¦ X, Y ì·¨ë“' ë“± ë§¤ìš° êµ¬ì²´ì ì´ê³ , ë‹¹ì¥ ì‹¤í–‰ ê°€ëŠ¥í•œ ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ í¬í•¨í•©ë‹ˆë‹¤.
- 3ì  (ë³´í†µ): 'ê²½í—˜ì„ ìŒ“ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤', 'ìê²©ì¦ì„ ì•Œì•„ë³´ì„¸ìš”'ì²˜ëŸ¼ ë°©í–¥ì„±ì€ ë§ì§€ë§Œ ë‹¤ì†Œ ì¼ë°˜ì ì´ê³  ì›ë¡ ì ì¸ ì¡°ì–¸ì…ë‹ˆë‹¤.
- 1ì  (ë§¤ìš° ë¯¸í¡): ë„ì›€ì´ ë˜ì§€ ì•Šê±°ë‚˜, ì§ˆë¬¸ê³¼ ë™ë¬¸ì„œë‹µí•˜ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.

ì ìˆ˜ (1-5ì ):
ì´ìœ :
"""

    def _create_profile_alignment_prompt(
        self, user_profile: Dict, generated_response: Dict
    ) -> str:
        """í”„ë¡œí•„ ì¼ì¹˜ë„ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        recommended_jobs = json.dumps(
            generated_response.get("recommended_jobs", []), ensure_ascii=False, indent=2
        )
        return f"""
ì¶”ì²œëœ 'ì±„ìš© ê³µê³  ë¦¬ìŠ¤íŠ¸'ê°€ 'ì‚¬ìš©ì í”„ë¡œí•„'ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
(ì´ í‰ê°€ëŠ” 'ì‚¬ìš©ì ì§ˆë¬¸'ì´ë‚˜ 'ìƒì„±ëœ ì¡°ì–¸'ì„ **ë¬´ì‹œ**í•˜ê³ , ì˜¤ì§ ì¶”ì²œëœ ê³µê³ ê°€ 'ì‚¬ìš©ì í”„ë¡œí•„'ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ì— ì§‘ì¤‘í•©ë‹ˆë‹¤.)

[ì •ë³´]
ì‚¬ìš©ì í”„ë¡œí•„:
- ì „ê³µ: {user_profile.get('major', 'N/A')}
- ìˆ˜ê°•ì´ë ¥: {user_profile.get('courses', [])}

[í‰ê°€ ëŒ€ìƒ]
ì¶”ì²œëœ ê³µê³ ë“¤:
{recommended_jobs}

[ì ìˆ˜ ê¸°ì¤€]
- 5ì  (ë§¤ìš° ìš°ìˆ˜): ëª¨ë“  ì¶”ì²œ ê³µê³ ê°€ ì‚¬ìš©ìì˜ ì „ê³µ ë˜ëŠ” ìˆ˜ê°•ì´ë ¥ ëª…í™•í•˜ê²Œ ì¼ì¹˜í•©ë‹ˆë‹¤.
- 3ì  (ë³´í†µ): ì¼ë¶€ ê³µê³ ëŠ” ì¼ì¹˜í•˜ì§€ë§Œ, ê´€ë ¨ì„±ì´ ë‚®ì€ ê³µê³ ê°€ 20-40% í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- 1ì  (ë§¤ìš° ë¯¸í¡): ì¶”ì²œëœ ê³µê³ ê°€ í”„ë¡œí•„ê³¼ ê±°ì˜ ë˜ëŠ” ì „í˜€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ì ìˆ˜ (1-5ì ):
ì´ìœ :
"""

    def _extract_score(self, response_text: str) -> float:
        """ì‘ë‹µì—ì„œ ì ìˆ˜ ì¶”ì¶œ"""
        try:
            lines = response_text.strip().split("\n")
            for line in lines:
                if "ì ìˆ˜" in line and ":" in line:
                    score_part = line.split(":")[1].strip()
                    import re

                    numbers = re.findall(r"\d+\.?\d*", score_part)
                    if numbers:
                        score = float(numbers[0])
                        return max(1.0, min(5.0, score))  # 1-5 ë²”ìœ„ë¡œ ì œí•œ
            return 3.0  # ê¸°ë³¸ê°’
        except:
            return 3.0

    def _extract_reasoning(self, response_text: str) -> str:
        """ì‘ë‹µì—ì„œ ì´ìœ  ì¶”ì¶œ"""
        try:
            lines = response_text.strip().split("\n")
            for line in lines:
                if "ì´ìœ " in line and ":" in line:
                    return line.split(":")[1].strip()
            return "ì´ìœ ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ"
        except:
            return "ì´ìœ  ì¶”ì¶œ ì‹¤íŒ¨"

    async def _create_langsmith_dataset(
        self, query_results: List[Dict[str, Any]], dataset_name: str
    ) -> str:
        """LangSmith ë°ì´í„°ì…‹ ìƒì„±"""

        try:
            # ê¸°ì¡´ ë°ì´í„°ì…‹ ì‚­ì œ (ìˆë‹¤ë©´)
            try:
                self.client.delete_dataset(dataset_name=dataset_name)
            except:
                pass

            # ìƒˆ ë°ì´í„°ì…‹ ìƒì„±
            dataset = self.client.create_dataset(
                dataset_name=dataset_name, description=f"Career-HY RAG í‰ê°€ìš© ë°ì´í„°ì…‹"
            )

            # ë°ì´í„° ì¶”ê°€ (ìƒˆë¡œìš´ API ë°©ì‹)
            examples = []
            for i, result in enumerate(query_results):
                example = self.client.create_example(
                    dataset_id=dataset.id,
                    inputs={
                        "query": result.get("query", ""),
                        "user_profile": result.get("user_profile", {}),
                        "retrieved_docs": result.get("retrieved_docs", []),
                        "generated_response": result.get("generated_response", {}),
                        "ground_truth_docs": result.get("ground_truth_docs", []),
                    },
                    outputs={"expected_quality": "high"},  # ê¸°ë³¸ê°’
                )
                examples.append(example)

            print(
                f"ğŸ“ LangSmith ë°ì´í„°ì…‹ ìƒì„±: {dataset_name} ({len(examples)}ê°œ í•­ëª©)"
            )
            return dataset_name

        except Exception as e:
            print(f"âŒ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    def _format_response_for_evaluation(self, inputs: Dict[str, Any]) -> str:
        """í‰ê°€ë¥¼ ìœ„í•œ ì‘ë‹µ í¬ë§·íŒ…"""
        generated_response = inputs.get("generated_response", {})
        return json.dumps(generated_response, ensure_ascii=False)

    def _aggregate_evaluation_results(
        self, evaluation_results: Dict[str, Any]
    ) -> List[LangSmithEvaluationResult]:
        """í‰ê°€ ê²°ê³¼ ì§‘ê³„"""

        final_results = []

        for metric_name, results in evaluation_results.items():
            if results is None:
                # í‰ê°€ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ê°’
                final_results.append(
                    LangSmithEvaluationResult(
                        metric_name=metric_name,
                        score=0.0,
                        reasoning="í‰ê°€ ì‹¤íŒ¨",
                        details={"error": "evaluation_failed"},
                    )
                )
                continue

            # ê²°ê³¼ì—ì„œ í‰ê·  ì ìˆ˜ ê³„ì‚°
            try:
                scores = []
                reasoning_samples = []

                for result in results:
                    if hasattr(result, "results") and result.results:
                        for eval_result in result.results:
                            if (
                                hasattr(eval_result, "score")
                                and eval_result.score is not None
                            ):
                                scores.append(float(eval_result.score))

                            if hasattr(eval_result, "comment") and eval_result.comment:
                                reasoning_samples.append(eval_result.comment)

                avg_score = sum(scores) / len(scores) if scores else 0.0
                sample_reasoning = (
                    reasoning_samples[0] if reasoning_samples else "í‰ê°€ ì™„ë£Œ"
                )

                final_results.append(
                    LangSmithEvaluationResult(
                        metric_name=metric_name,
                        score=avg_score,
                        reasoning=sample_reasoning,
                        details={
                            "total_evaluations": len(scores),
                            "score_distribution": {
                                "min": min(scores) if scores else 0,
                                "max": max(scores) if scores else 0,
                                "avg": avg_score,
                            },
                        },
                    )
                )

            except Exception as e:
                print(f"âš ï¸  {metric_name} ê²°ê³¼ ì§‘ê³„ ì‹¤íŒ¨: {e}")
                final_results.append(
                    LangSmithEvaluationResult(
                        metric_name=metric_name,
                        score=0.0,
                        reasoning=f"ì§‘ê³„ ì‹¤íŒ¨: {e}",
                        details={"error": "aggregation_failed"},
                    )
                )

        return final_results
