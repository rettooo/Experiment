"""
LangSmith ê¸°ë°˜ ê³ í’ˆì§ˆ ì‘ë‹µ í‰ê°€

LLM-as-Judge ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„± í’ˆì§ˆì„ ì •êµí•˜ê²Œ í‰ê°€
"""

import json
import asyncio
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

from langsmith import Client, traceable
from langsmith.evaluation import evaluate
from langchain_openai import ChatOpenAI


@dataclass
class LangSmithEvaluationResult:
    """LangSmith í‰ê°€ ê²°ê³¼"""
    metric_name: str
    score: float
    reasoning: str
    details: Dict[str, Any]


class CareerHYLangSmithEvaluator:
    """Career-HY ë§ì¶¤í˜• LangSmith í‰ê°€ê¸°"""

    def __init__(
        self,
        judge_model: str = "gpt-4o-mini",
        project_name: str = "career-hy-rag-evaluation"
    ):
        self.judge_model = judge_model
        self.project_name = project_name
        self.client = Client()
        self.llm = ChatOpenAI(model=judge_model, temperature=0.1)

        # í‰ê°€ ì§€í‘œë“¤
        self.metrics = [
            "recommendation_quality",
            "personalization_score",
            "response_helpfulness",
            "profile_alignment"
        ]

    async def evaluate_batch(
        self,
        query_results: List[Dict[str, Any]],
        experiment_name: str
    ) -> List[LangSmithEvaluationResult]:
        """
        ì—¬ëŸ¬ ì¿¼ë¦¬ ê²°ê³¼ì— ëŒ€í•œ LangSmith ë°°ì¹˜ í‰ê°€

        Args:
            query_results: ì¿¼ë¦¬ë³„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            experiment_name: ì‹¤í—˜ëª… (LangSmith ì¶”ì ìš©)

        Returns:
            í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """

        print(f"ğŸ” LangSmith í‰ê°€ ì‹œì‘: {len(query_results)}ê°œ ì¿¼ë¦¬")
        print(f"ğŸ¤– Judge ëª¨ë¸: {self.judge_model}")
        print(f"ğŸ“Š í‰ê°€ ì§€í‘œ: {self.metrics}")

        # ì§ì ‘ í‰ê°€ ì‹¤í–‰
        final_results = []

        for metric_name in self.metrics:
            print(f"\nğŸ“ˆ {metric_name} í‰ê°€ ì¤‘...")

            try:
                scores = []
                reasonings = []

                for i, result in enumerate(query_results):
                    evaluation_result = await self._evaluate_single_query(result, metric_name)
                    scores.append(evaluation_result["score"])
                    reasonings.append(evaluation_result["reasoning"])

                avg_score = sum(scores) / len(scores) if scores else 0.0

                final_results.append(LangSmithEvaluationResult(
                    metric_name=metric_name,
                    score=avg_score,
                    reasoning=f"í‰ê·  ì ìˆ˜: {avg_score:.3f}",
                    details={
                        "individual_scores": scores,
                        "individual_reasonings": reasonings,
                        "total_queries": len(query_results)
                    }
                ))

                print(f"âœ… {metric_name} í‰ê°€ ì™„ë£Œ: {avg_score:.3f}")

            except Exception as e:
                print(f"âŒ {metric_name} í‰ê°€ ì‹¤íŒ¨: {e}")
                final_results.append(LangSmithEvaluationResult(
                    metric_name=metric_name,
                    score=0.0,
                    reasoning=f"í‰ê°€ ì‹¤íŒ¨: {e}",
                    details={"error": str(e)}
                ))

        print(f"\nğŸ‰ LangSmith í‰ê°€ ì™„ë£Œ!")
        for result in final_results:
            print(f"  {result.metric_name}: {result.score:.3f}")

        return final_results

    async def _evaluate_single_query(self, query_result: Dict[str, Any], metric_name: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ íŠ¹ì • ì§€í‘œ í‰ê°€"""

        try:
            query = query_result.get('query', '')
            user_profile = query_result.get('user_profile', {})
            generated_response = query_result.get('generated_response', {})

            # ë©”íŠ¸ë¦­ë³„ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±
            if metric_name == "recommendation_quality":
                prompt = self._create_recommendation_quality_prompt(query, user_profile, generated_response)
            elif metric_name == "personalization_score":
                prompt = self._create_personalization_prompt(query, user_profile, generated_response)
            elif metric_name == "response_helpfulness":
                prompt = self._create_helpfulness_prompt(query, user_profile, generated_response)
            elif metric_name == "profile_alignment":
                prompt = self._create_profile_alignment_prompt(query, user_profile, generated_response)
            else:
                raise ValueError(f"Unknown metric: {metric_name}")

            # LLMìœ¼ë¡œ í‰ê°€ ì‹¤í–‰
            response = await self.llm.ainvoke(prompt)
            response_text = response.content

            # ì ìˆ˜ì™€ ì´ìœ  ì¶”ì¶œ
            score = self._extract_score(response_text)
            reasoning = self._extract_reasoning(response_text)

            return {
                "score": score,
                "reasoning": reasoning,
                "full_response": response_text
            }

        except Exception as e:
            return {
                "score": 0.0,
                "reasoning": f"í‰ê°€ ì‹¤íŒ¨: {e}",
                "full_response": str(e)
            }

    def _create_recommendation_quality_prompt(self, query: str, user_profile: Dict, generated_response: Dict) -> str:
        """ì¶”ì²œ í’ˆì§ˆ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return f"""
ë‹¤ìŒ ì±„ìš©ê³µê³  ì¶”ì²œì˜ í’ˆì§ˆì„ 1-5ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ì‚¬ìš©ì í”„ë¡œí•„:
- ì „ê³µ: {user_profile.get('major', 'N/A')}
- ê´€ì‹¬ ì§ë¬´: {user_profile.get('interest_job', 'N/A')}

ìƒì„±ëœ ì¶”ì²œ:
{json.dumps(generated_response.get('recommended_jobs', []), ensure_ascii=False, indent=2)}

í‰ê°€ ê¸°ì¤€:
1. ê´€ë ¨ì„±: ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì¶”ì²œ ê³µê³ ì˜ ì—°ê´€ì„±
2. ê°œì¸í™”: ì‚¬ìš©ì í”„ë¡œí•„ê³¼ ì¶”ì²œì˜ ë§ì¶¤ì„±
3. êµ¬ì²´ì„±: ì¶”ì²œ ì´ìœ ì˜ ìƒì„¸í•¨ê³¼ ìœ ìš©ì„±

ì ìˆ˜ (1-5ì ):
ì´ìœ :
"""

    def _create_personalization_prompt(self, query: str, user_profile: Dict, generated_response: Dict) -> str:
        """ê°œì¸í™” í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return f"""
ë‹¤ìŒ ì¶”ì²œì´ ì‚¬ìš©ìì—ê²Œ ì–¼ë§ˆë‚˜ ê°œì¸í™”ë˜ì–´ ìˆëŠ”ì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì í”„ë¡œí•„:
- ì „ê³µ: {user_profile.get('major', 'N/A')}
- ê´€ì‹¬ ì§ë¬´: {user_profile.get('interest_job', 'N/A')}

ì¶”ì²œ ì‘ë‹µ:
{generated_response.get('content', '')}

í‰ê°€ ê¸°ì¤€: í”„ë¡œí•„ ìš”ì†Œê°€ ì¶”ì²œì— ì–¼ë§ˆë‚˜ ì˜ ë°˜ì˜ë˜ì—ˆëŠ”ê°€

ì ìˆ˜ (1-5ì ):
ì´ìœ :
"""

    def _create_helpfulness_prompt(self, query: str, user_profile: Dict, generated_response: Dict) -> str:
        """ë„ì›€ì´ ë˜ëŠ” ì •ë„ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return f"""
ë‹¤ìŒ ì¶”ì²œì´ ì·¨ì—… ì¤€ë¹„ìƒì—ê²Œ ì–¼ë§ˆë‚˜ ë„ì›€ì´ ë˜ëŠ”ì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ì¶”ì²œ ì‘ë‹µ:
{generated_response.get('content', '')}

í‰ê°€ ê¸°ì¤€: ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì¸ ì¡°ì–¸ ì œê³µ ì—¬ë¶€

ì ìˆ˜ (1-5ì ):
ì´ìœ :
"""

    def _create_profile_alignment_prompt(self, query: str, user_profile: Dict, generated_response: Dict) -> str:
        """í”„ë¡œí•„ ì¼ì¹˜ë„ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return f"""
ë‹¤ìŒ ì¶”ì²œì´ ì‚¬ìš©ì í”„ë¡œí•„ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ 1-5ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì í”„ë¡œí•„:
- ì „ê³µ: {user_profile.get('major', 'N/A')}
- ê´€ì‹¬ ì§ë¬´: {user_profile.get('interest_job', 'N/A')}

ì¶”ì²œëœ ê³µê³ ë“¤:
{json.dumps(generated_response.get('recommended_jobs', []), ensure_ascii=False, indent=2)}

í‰ê°€ ê¸°ì¤€: ì¶”ì²œ ê³µê³ ë“¤ì´ ì‚¬ìš©ì ë°°ê²½ê³¼ ì–¼ë§ˆë‚˜ ì˜ ë§ëŠ”ê°€

ì ìˆ˜ (1-5ì ):
ì´ìœ :
"""

    def _extract_score(self, response_text: str) -> float:
        """ì‘ë‹µì—ì„œ ì ìˆ˜ ì¶”ì¶œ"""
        try:
            lines = response_text.strip().split('\n')
            for line in lines:
                if 'ì ìˆ˜' in line and ':' in line:
                    score_part = line.split(':')[1].strip()
                    import re
                    numbers = re.findall(r'\d+\.?\d*', score_part)
                    if numbers:
                        score = float(numbers[0])
                        return max(1.0, min(5.0, score))  # 1-5 ë²”ìœ„ë¡œ ì œí•œ
            return 3.0  # ê¸°ë³¸ê°’
        except:
            return 3.0

    def _extract_reasoning(self, response_text: str) -> str:
        """ì‘ë‹µì—ì„œ ì´ìœ  ì¶”ì¶œ"""
        try:
            lines = response_text.strip().split('\n')
            for line in lines:
                if 'ì´ìœ ' in line and ':' in line:
                    return line.split(':')[1].strip()
            return "ì´ìœ ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ"
        except:
            return "ì´ìœ  ì¶”ì¶œ ì‹¤íŒ¨"

    async def _create_langsmith_dataset(
        self,
        query_results: List[Dict[str, Any]],
        dataset_name: str
    ) -> str:
        """LangSmith ë°ì´í„°ì…‹ ìƒì„±"""

        try:
            # ê¸°ì¡´ ë°ì´í„°ì…‹ ì‚­ì œ (ìˆë‹¤ë©´)
            try:
                self.client.delete_dataset(dataset_name=dataset_name)
            except:
                pass

            # ìƒˆ ë°ì´í„°ì…‹ ìƒì„±
            dataset = self.client.create_dataset(
                dataset_name=dataset_name,
                description=f"Career-HY RAG í‰ê°€ìš© ë°ì´í„°ì…‹"
            )

            # ë°ì´í„° ì¶”ê°€ (ìƒˆë¡œìš´ API ë°©ì‹)
            examples = []
            for i, result in enumerate(query_results):
                example = self.client.create_example(
                    dataset_id=dataset.id,
                    inputs={
                        "query": result.get('query', ''),
                        "user_profile": result.get('user_profile', {}),
                        "retrieved_docs": result.get('retrieved_docs', []),
                        "generated_response": result.get('generated_response', {}),
                        "ground_truth_docs": result.get('ground_truth_docs', [])
                    },
                    outputs={
                        "expected_quality": "high"  # ê¸°ë³¸ê°’
                    }
                )
                examples.append(example)

            print(f"ğŸ“ LangSmith ë°ì´í„°ì…‹ ìƒì„±: {dataset_name} ({len(examples)}ê°œ í•­ëª©)")
            return dataset_name

        except Exception as e:
            print(f"âŒ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    def _format_response_for_evaluation(self, inputs: Dict[str, Any]) -> str:
        """í‰ê°€ë¥¼ ìœ„í•œ ì‘ë‹µ í¬ë§·íŒ…"""
        generated_response = inputs.get('generated_response', {})
        return json.dumps(generated_response, ensure_ascii=False)

    def _aggregate_evaluation_results(
        self,
        evaluation_results: Dict[str, Any]
    ) -> List[LangSmithEvaluationResult]:
        """í‰ê°€ ê²°ê³¼ ì§‘ê³„"""

        final_results = []

        for metric_name, results in evaluation_results.items():
            if results is None:
                # í‰ê°€ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ê°’
                final_results.append(LangSmithEvaluationResult(
                    metric_name=metric_name,
                    score=0.0,
                    reasoning="í‰ê°€ ì‹¤íŒ¨",
                    details={"error": "evaluation_failed"}
                ))
                continue

            # ê²°ê³¼ì—ì„œ í‰ê·  ì ìˆ˜ ê³„ì‚°
            try:
                scores = []
                reasoning_samples = []

                for result in results:
                    if hasattr(result, 'results') and result.results:
                        for eval_result in result.results:
                            if hasattr(eval_result, 'score') and eval_result.score is not None:
                                scores.append(float(eval_result.score))

                            if hasattr(eval_result, 'comment') and eval_result.comment:
                                reasoning_samples.append(eval_result.comment)

                avg_score = sum(scores) / len(scores) if scores else 0.0
                sample_reasoning = reasoning_samples[0] if reasoning_samples else "í‰ê°€ ì™„ë£Œ"

                final_results.append(LangSmithEvaluationResult(
                    metric_name=metric_name,
                    score=avg_score,
                    reasoning=sample_reasoning,
                    details={
                        "total_evaluations": len(scores),
                        "score_distribution": {
                            "min": min(scores) if scores else 0,
                            "max": max(scores) if scores else 0,
                            "avg": avg_score
                        }
                    }
                ))

            except Exception as e:
                print(f"âš ï¸  {metric_name} ê²°ê³¼ ì§‘ê³„ ì‹¤íŒ¨: {e}")
                final_results.append(LangSmithEvaluationResult(
                    metric_name=metric_name,
                    score=0.0,
                    reasoning=f"ì§‘ê³„ ì‹¤íŒ¨: {e}",
                    details={"error": "aggregation_failed"}
                ))

        return final_results


