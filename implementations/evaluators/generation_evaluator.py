"""
ìë™í™”ëœ ì‘ë‹µ ìƒì„± í’ˆì§ˆ í‰ê°€

LLM Judge ì—†ì´ ìë™í™”ëœ ì§€í‘œë“¤ë¡œ ì‘ë‹µ ìƒì„± í’ˆì§ˆì„ ì¸¡ì •
ë¹„ìš© íš¨ìœ¨ì ì´ë©´ì„œë„ ì˜ë¯¸ ìˆëŠ” í‰ê°€ë¥¼ ì œê³µ
"""

import re
import json
from typing import Dict, List, Any, Optional
from dataclasses import dataclass


@dataclass
class GenerationEvaluationResult:
    """ìƒì„± í‰ê°€ ê²°ê³¼"""
    metric_name: str
    score: float
    details: Dict[str, Any]


class GenerationEvaluator:
    """ìë™í™”ëœ ì‘ë‹µ ìƒì„± í’ˆì§ˆ í‰ê°€ê¸°"""

    def __init__(self):
        self.metrics = [
            "recommendation_accuracy",
            "profile_utilization",
            "response_completeness",
            "structure_quality"
        ]

    def evaluate_batch(
        self,
        query_results: List[Dict[str, Any]]
    ) -> List[GenerationEvaluationResult]:
        """
        ì—¬ëŸ¬ ì¿¼ë¦¬ ê²°ê³¼ì— ëŒ€í•œ ë°°ì¹˜ í‰ê°€

        Args:
            query_results: ì¿¼ë¦¬ë³„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
                ê° í•­ëª©: {
                    'query': str,
                    'user_profile': dict,
                    'ground_truth_docs': list,
                    'generated_response': dict,
                    'retrieved_docs': list
                }

        Returns:
            í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """

        print(f"ğŸ” ìƒì„± í’ˆì§ˆ í‰ê°€ ì‹œì‘: {len(query_results)}ê°œ ì¿¼ë¦¬")

        # ê° ì§€í‘œë³„ ì ìˆ˜ ìˆ˜ì§‘
        metric_scores = {metric: [] for metric in self.metrics}
        detailed_results = []

        for i, result in enumerate(query_results):
            try:
                # ê°œë³„ ì¿¼ë¦¬ í‰ê°€
                individual_scores = self._evaluate_single_query(result)

                # ì ìˆ˜ ìˆ˜ì§‘
                for metric, score in individual_scores.items():
                    if metric in metric_scores:
                        metric_scores[metric].append(score)

                detailed_results.append({
                    'query_index': i,
                    'query': result['query'][:50] + '...',
                    'scores': individual_scores
                })

                if (i + 1) % 10 == 0:
                    print(f"  í‰ê°€ ì§„í–‰ë¥ : {i + 1}/{len(query_results)}")

            except Exception as e:
                print(f"  âš ï¸  ì¿¼ë¦¬ {i} í‰ê°€ ì‹¤íŒ¨: {e}")
                # ê¸°ë³¸ ì ìˆ˜ë¡œ ì±„ì›€
                for metric in self.metrics:
                    metric_scores[metric].append(0.0)

        # í‰ê·  ì ìˆ˜ ê³„ì‚° ë° ê²°ê³¼ ìƒì„±
        evaluation_results = []
        for metric in self.metrics:
            scores = metric_scores[metric]
            avg_score = sum(scores) / len(scores) if scores else 0.0

            evaluation_results.append(GenerationEvaluationResult(
                metric_name=metric,
                score=avg_score,
                details={
                    'total_queries': len(query_results),
                    'successful_evaluations': len([s for s in scores if s > 0]),
                    'score_distribution': {
                        'min': min(scores) if scores else 0,
                        'max': max(scores) if scores else 0,
                        'avg': avg_score
                    }
                }
            ))

        print(f"âœ… ìƒì„± í’ˆì§ˆ í‰ê°€ ì™„ë£Œ")
        for result in evaluation_results:
            print(f"  {result.metric_name}: {result.score:.4f}")

        return evaluation_results

    def _evaluate_single_query(self, result: Dict[str, Any]) -> Dict[str, float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ í‰ê°€"""

        query = result.get('query', '')
        user_profile = result.get('user_profile', {})
        ground_truth_docs = result.get('ground_truth_docs', [])
        generated_response = result.get('generated_response', {})
        retrieved_docs = result.get('retrieved_docs', [])

        scores = {}

        # 1. ì¶”ì²œ ì •í™•ë„
        scores['recommendation_accuracy'] = self._calculate_recommendation_accuracy(
            generated_response, ground_truth_docs
        )

        # 2. í”„ë¡œí•„ í™œìš©ë„
        scores['profile_utilization'] = self._calculate_profile_utilization(
            generated_response, user_profile
        )

        # 3. ì‘ë‹µ ì™„ì„±ë„
        scores['response_completeness'] = self._calculate_response_completeness(
            generated_response, query
        )

        # 4. êµ¬ì¡° í’ˆì§ˆ
        scores['structure_quality'] = self._calculate_structure_quality(
            generated_response
        )

        return scores

    def _calculate_recommendation_accuracy(
        self,
        generated_response: Dict[str, Any],
        ground_truth_docs: List[str]
    ) -> float:
        """ì¶”ì²œ ì •í™•ë„: ì¶”ì²œëœ ê³µê³  ì¤‘ ì‹¤ì œ ê´€ë ¨ ê³µê³  ë¹„ìœ¨"""

        try:
            recommended_jobs = generated_response.get('recommended_jobs', [])

            if not recommended_jobs:
                return 0.0

            if not ground_truth_docs:
                return 0.5  # GTê°€ ì—†ìœ¼ë©´ ì¤‘ë¦½ ì ìˆ˜

            # ì¶”ì²œëœ ê³µê³ ì˜ rec_idx ì¶”ì¶œ
            recommended_rec_ids = []
            for job in recommended_jobs:
                rec_idx = job.get('rec_idx')
                if rec_idx:
                    recommended_rec_ids.append(str(rec_idx))

            if not recommended_rec_ids:
                return 0.0

            # GTì™€ ì¶”ì²œ ê²°ê³¼ì˜ êµì§‘í•© ê³„ì‚°
            gt_set = set(str(doc) for doc in ground_truth_docs)
            recommended_set = set(recommended_rec_ids)

            intersection = gt_set & recommended_set
            accuracy = len(intersection) / len(recommended_set)

            return accuracy

        except Exception as e:
            print(f"ì¶”ì²œ ì •í™•ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def _calculate_profile_utilization(
        self,
        generated_response: Dict[str, Any],
        user_profile: Dict[str, Any]
    ) -> float:
        """í”„ë¡œí•„ í™œìš©ë„: ì‘ë‹µì—ì„œ ì‚¬ìš©ì í”„ë¡œí•„ ìš”ì†Œ ì–¸ê¸‰ ë¹„ìœ¨"""

        try:
            response_content = generated_response.get('content', '')
            if not response_content:
                return 0.0

            response_lower = response_content.lower()

            # ì²´í¬í•  í”„ë¡œí•„ ìš”ì†Œë“¤
            profile_elements = []

            # ì „ê³µ
            major = user_profile.get('major', '')
            if major:
                profile_elements.append(('major', major))

            # ê´€ì‹¬ ì§ë¬´
            interest_jobs = user_profile.get('interest_job', [])
            if isinstance(interest_jobs, list):
                for job in interest_jobs:
                    profile_elements.append(('interest_job', job))
            elif interest_jobs:
                profile_elements.append(('interest_job', interest_jobs))

            # ìê²©ì¦
            certifications = user_profile.get('certification', [])
            if isinstance(certifications, list):
                for cert in certifications:
                    profile_elements.append(('certification', cert))
            elif certifications:
                profile_elements.append(('certification', certifications))

            # ìˆ˜ê°• ê³¼ëª© (catalogsì—ì„œ ì¶”ì¶œ)
            catalogs = user_profile.get('catalogs', [])
            if isinstance(catalogs, list):
                for catalog in catalogs[:5]:  # ìƒìœ„ 5ê°œë§Œ ì²´í¬
                    if isinstance(catalog, dict):
                        course_name = catalog.get('course_name', '')
                        if course_name:
                            profile_elements.append(('course', course_name))

            if not profile_elements:
                return 0.5  # í”„ë¡œí•„ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì¤‘ë¦½ ì ìˆ˜

            # ì–¸ê¸‰ëœ ìš”ì†Œ ì¹´ìš´íŠ¸
            mentioned_count = 0
            for element_type, element_value in profile_elements:
                if self._is_mentioned_in_text(element_value, response_lower):
                    mentioned_count += 1

            utilization_score = mentioned_count / len(profile_elements)
            return utilization_score

        except Exception as e:
            print(f"í”„ë¡œí•„ í™œìš©ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def _calculate_response_completeness(
        self,
        generated_response: Dict[str, Any],
        query: str
    ) -> float:
        """ì‘ë‹µ ì™„ì„±ë„: ì‘ë‹µì´ ì–¼ë§ˆë‚˜ ì™„ì „í•œê°€"""

        try:
            content = generated_response.get('content', '')
            recommended_jobs = generated_response.get('recommended_jobs', [])

            score_components = []

            # 1. ë‚´ìš© ì¡´ì¬ ì—¬ë¶€ (0.3)
            if content and len(content.strip()) >= 20:
                score_components.append(0.3)
            else:
                score_components.append(0.0)

            # 2. ì¶”ì²œ ê³µê³  ì¡´ì¬ ì—¬ë¶€ (0.3)
            if recommended_jobs and len(recommended_jobs) > 0:
                score_components.append(0.3)
            else:
                score_components.append(0.0)

            # 3. ì¶”ì²œ ì´ìœ  ì¡´ì¬ ì—¬ë¶€ (0.4)
            reason_score = 0.0
            for job in recommended_jobs:
                reason = job.get('recommendation_reason', '')
                if reason and len(reason.strip()) >= 10:
                    reason_score += 0.4 / len(recommended_jobs)

            score_components.append(min(0.4, reason_score))

            return sum(score_components)

        except Exception as e:
            print(f"ì‘ë‹µ ì™„ì„±ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def _calculate_structure_quality(self, generated_response: Dict[str, Any]) -> float:
        """êµ¬ì¡° í’ˆì§ˆ: JSON ì‘ë‹µ êµ¬ì¡°ì˜ ì™„ì„±ë„"""

        try:
            score = 0.0

            # 1. ê¸°ë³¸ í•„ë“œ ì¡´ì¬ (0.5)
            if 'content' in generated_response:
                score += 0.25
            if 'recommended_jobs' in generated_response:
                score += 0.25

            # 2. recommended_jobs êµ¬ì¡° í’ˆì§ˆ (0.5)
            recommended_jobs = generated_response.get('recommended_jobs', [])
            if recommended_jobs:
                valid_jobs = 0
                required_fields = ['rec_idx', 'title', 'url', 'recommendation_reason']

                for job in recommended_jobs:
                    if isinstance(job, dict):
                        field_count = sum(1 for field in required_fields if field in job and job[field])
                        if field_count >= 3:  # ìµœì†Œ 3ê°œ í•„ë“œëŠ” ìˆì–´ì•¼ í•¨
                            valid_jobs += 1

                if recommended_jobs:
                    job_structure_score = valid_jobs / len(recommended_jobs)
                    score += 0.5 * job_structure_score

            return score

        except Exception as e:
            print(f"êµ¬ì¡° í’ˆì§ˆ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def _is_mentioned_in_text(self, element: str, text: str) -> bool:
        """í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • ìš”ì†Œê°€ ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€ í™•ì¸"""

        if not element or not text:
            return False

        element_lower = element.lower()

        # ì •í™•í•œ ë§¤ì¹˜
        if element_lower in text:
            return True

        # í‚¤ì›Œë“œ ë§¤ì¹˜ (ê³µë°± ì œê±° í›„)
        element_keywords = element_lower.replace(' ', '').replace('-', '')
        text_normalized = text.replace(' ', '').replace('-', '')

        if element_keywords in text_normalized:
            return True

        # ë¶€ë¶„ ë§¤ì¹˜ (ê¸¸ì´ê°€ 3ì ì´ìƒì¸ ê²½ìš°)
        if len(element_keywords) >= 3:
            words = element_keywords.split()
            for word in words:
                if len(word) >= 3 and word in text:
                    return True

        return False