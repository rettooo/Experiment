"""
Career-HY RAG ì‹¤í—˜ íŒŒì´í”„ë¼ì¸ ë©”ì¸ ë¡œì§
"""

import json
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

from .config import ExperimentConfig
from .interfaces.evaluator import QueryResult
from utils.factory import ComponentFactory
from utils.data_loader import S3DataLoader
from utils.embedding_cache import embedding_cache
from implementations.evaluators import SearchMetricsEvaluator


class ExperimentPipeline:
    """RAG ì‹¤í—˜ì„ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ íŒŒì´í”„ë¼ì¸"""

    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.results = {}

        # ì‹¤í—˜ ID ë° ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.experiment_id = config.get_experiment_id()
        self.output_dir = config.get_output_path()

        print(f"ì‹¤í—˜ ì‹œì‘: {config.experiment_name}")
        print(f"ì‹¤í—˜ ID: {self.experiment_id}")
        print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")

    def run(self) -> Dict[str, Any]:
        """
        ì „ì²´ ì‹¤í—˜ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Returns:
            ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()

        try:
            # 1. ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
            print("\n=== 1. ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ===")
            components = self._initialize_components()

            # 2. ë°ì´í„° ë¡œë“œ
            print("\n=== 2. ë°ì´í„° ë¡œë“œ ===")
            # S3ì˜ ëª¨ë“  ë°ì´í„° ì‚¬ìš©
            documents = self._load_documents()

            # 3. ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ë² ë”©
            print("\n=== 3. ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ë² ë”© ===")
            processed_docs, embeddings = self._process_documents(documents, components)

            # 4. ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶•
            print("\n=== 4. ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶• ===")
            self._build_retrieval_system(processed_docs, embeddings, components)

            # 5. Ground Truth ì¿¼ë¦¬ ë¡œë“œ
            print("\n=== 5. Ground Truth ì¿¼ë¦¬ ë¡œë“œ ===")
            test_queries = self._load_test_queries()

            # 6. ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€
            print("\n=== 6. ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ ===")
            query_results = self._evaluate_retrieval(test_queries, components)

            # 7. ê²°ê³¼ ì €ì¥
            print("\n=== 7. ê²°ê³¼ ì €ì¥ ===")
            results = self._save_results(query_results, components, start_time)

            print(f"\nì‹¤í—˜ ì™„ë£Œ! ì´ ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
            return results

        except Exception as e:
            print(f"\nì‹¤í—˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    def _initialize_components(self) -> Dict[str, Any]:
        """ì„¤ì •ì— ë”°ë¼ ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”"""
        components = {}

        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        print(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”: {self.config.embedder.type} - {self.config.embedder.model_name}")
        components['embedder'] = ComponentFactory.create_embedder(self.config.embedder)

        # ì²­í‚¹ ì „ëµ ì´ˆê¸°í™”
        print(f"ì²­í‚¹ ì „ëµ ì´ˆê¸°í™”: {self.config.chunker.type}")
        components['chunker'] = ComponentFactory.create_chunker(self.config.chunker)

        # ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print(f"ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”: {self.config.retriever.type}")
        components['retriever'] = ComponentFactory.create_retriever(self.config.retriever)

        # LLM ëª¨ë¸ ì´ˆê¸°í™” (í•„ìš”í•œ ê²½ìš°)
        if hasattr(self.config, 'llm') and self.config.llm:
            print(f"LLM ëª¨ë¸ ì´ˆê¸°í™”: {self.config.llm.type} - {self.config.llm.model_name}")
            components['llm'] = ComponentFactory.create_llm(self.config.llm)

        # í‰ê°€ê¸° ì´ˆê¸°í™”
        components['evaluator'] = SearchMetricsEvaluator(
            k_values=self.config.evaluation.k_values
        )

        return components

    def _load_documents(self) -> List[Dict[str, Any]]:
        """S3ì—ì„œ ëª¨ë“  ë¬¸ì„œ ë°ì´í„° ë¡œë“œ"""
        data_loader = S3DataLoader(bucket_name=self.config.data.s3_bucket)

        documents = data_loader.load_documents(
            pdf_prefix=self.config.data.pdf_prefix,
            json_prefix=self.config.data.json_prefix
        )

        print(f"ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
        return documents

    def _process_documents(self, documents: List[Dict[str, Any]], components: Dict[str, Any]) -> tuple:
        """ë¬¸ì„œ ì²­í‚¹ ë° ì„ë² ë”© ì²˜ë¦¬ (ìºì‹± ì§€ì›)"""
        chunker = components['chunker']
        embedder = components['embedder']

        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = embedding_cache.generate_cache_key(
            self.config.embedder,
            self.config.chunker
        )

        # ìºì‹œ í™•ì¸
        if embedding_cache.exists(cache_key):
            print(f"âœ… ê¸°ì¡´ ì„ë² ë”© ìºì‹œ ì‚¬ìš©: {cache_key}")
            cached_documents, cached_embeddings = embedding_cache.load(cache_key)
            return cached_documents, cached_embeddings

        print(f"ğŸ”„ ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„±: {cache_key}")

        all_chunks = []
        all_texts = []

        print("ë¬¸ì„œ ì²­í‚¹ ì¤‘...")
        for i, doc in enumerate(documents):
            # ì²­í‚¹ ìˆ˜í–‰
            chunks = chunker.chunk(doc['text'], doc['metadata'])
            all_chunks.extend(chunks)

            # ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for chunk in chunks:
                all_texts.append(chunk['text'])

            if (i + 1) % 50 == 0:
                print(f"ì²­í‚¹ ì™„ë£Œ: {i + 1}/{len(documents)} ë¬¸ì„œ")

        print(f"ì´ ì²­í¬ ìˆ˜: {len(all_chunks)}")

        # ì„ë² ë”© ìƒì„±
        print("ì„ë² ë”© ìƒì„± ì¤‘...")
        embeddings = embedder.embed(all_texts)

        print(f"ì„ë² ë”© ì™„ë£Œ: {len(embeddings)}ê°œ ë²¡í„°")

        # ìºì‹œì— ì €ì¥
        additional_info = {
            "original_document_count": len(documents),
            "embedder_config": self.config.embedder.__dict__,
            "chunker_config": self.config.chunker.__dict__
        }

        try:
            embedding_cache.save(cache_key, all_chunks, embeddings, additional_info)
        except Exception as e:
            print(f"âš ï¸  ìºì‹œ ì €ì¥ ì‹¤íŒ¨ (ì‹¤í—˜ì€ ê³„ì† ì§„í–‰): {e}")

        return all_chunks, embeddings

    def _build_retrieval_system(self, documents: List[Dict[str, Any]], embeddings: List[List[float]], components: Dict[str, Any]) -> None:
        """ê²€ìƒ‰ ì‹œìŠ¤í…œì— ë¬¸ì„œì™€ ì„ë² ë”© ì¶”ê°€"""
        retriever = components['retriever']

        print("ê²€ìƒ‰ ì‹œìŠ¤í…œì— ë¬¸ì„œ ì¶”ê°€ ì¤‘...")

        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (ì‹¤í—˜ìš©)
        if hasattr(retriever, 'clear_collection'):
            retriever.clear_collection()

        # ë¬¸ì„œì™€ ì„ë² ë”©ì„ ê²€ìƒ‰ ì‹œìŠ¤í…œì— ì¶”ê°€
        retriever.add_documents(documents, embeddings)

        doc_count = retriever.get_document_count()
        print(f"ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ: {doc_count}ê°œ ë¬¸ì„œ")

    def _load_test_queries(self) -> List[Dict[str, Any]]:
        """Ground Truth í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ë¡œë“œ"""
        test_queries_path = self.config.data.test_queries_path

        if not Path(test_queries_path).exists():
            print(f"âš ï¸  í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {test_queries_path}")
            print("ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            return self._create_sample_queries()

        queries = []
        with open(test_queries_path, 'r', encoding='utf-8') as f:
            for line in f:
                query_data = json.loads(line.strip())
                queries.append(query_data)

        print(f"í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ë¡œë“œ ì™„ë£Œ: {len(queries)}ê°œ")
        return queries

    def _create_sample_queries(self) -> List[Dict[str, Any]]:
        """ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„± (Ground Truthê°€ ì—†ì„ ë•Œ)"""
        sample_queries = [
            {
                "query": "ì»´í“¨í„°ê³µí•™ ì „ê³µ ì‹ ì… ê°œë°œì ì±„ìš©ê³µê³ ",
                "ground_truth_docs": [],  # ì‹¤ì œë¡œëŠ” ê´€ë ¨ ë¬¸ì„œ IDë“¤ì´ ë“¤ì–´ê°€ì•¼ í•¨
                "user_profile": {
                    "major": "ì»´í“¨í„°ê³µí•™ê³¼",
                    "interest_job": ["ê°œë°œì", "í”„ë¡œê·¸ë˜ë¨¸"]
                }
            },
            {
                "query": "ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ê´€ë ¨ ì§ë¬´",
                "ground_truth_docs": [],
                "user_profile": {
                    "major": "ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤í•™ê³¼",
                    "interest_job": ["ë°ì´í„° ë¶„ì„ê°€", "ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸"]
                }
            }
        ]

        print(f"ìƒ˜í”Œ ì¿¼ë¦¬ ìƒì„±: {len(sample_queries)}ê°œ")
        return sample_queries

    def count_tokens(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚° (tiktoken ì‚¬ìš©)"""
        try:
            import tiktoken
            encoding = tiktoken.encoding_for_model("text-embedding-ada-002")
            return len(encoding.encode(text))
        except Exception as e:
            print(f"í† í° ì¹´ìš´íŒ… ì‹¤íŒ¨: {e}")
            # ëŒ€ëµì ì¸ ì¶”ì • (1 í† í° â‰ˆ 4 ê¸€ì)
            return len(text) // 4

    def trim_courses_if_needed(self, query_text: str, max_tokens: int = 7500) -> str:
        """í† í° ì´ˆê³¼ ì‹œ ìˆ˜ê°• ì´ë ¥ì—ì„œ ê³¼ëª©ì„ ìˆœì°¨ì ìœ¼ë¡œ ì œê±°"""

        # 1. í˜„ì¬ í† í° ìˆ˜ ì²´í¬
        if self.count_tokens(query_text) <= max_tokens:
            return query_text

        # 2. ìˆ˜ê°• ì´ë ¥ ë¶€ë¶„ ë¶„ë¦¬
        lines = query_text.split('\n')
        course_history_start = -1

        for i, line in enumerate(lines):
            if line.startswith('ìˆ˜ê°• ì´ë ¥:'):
                course_history_start = i
                break

        if course_history_start == -1:
            return query_text  # ìˆ˜ê°• ì´ë ¥ì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

        # 3. ê¸°ë³¸ ì •ë³´ ë¶€ë¶„ê³¼ ìˆ˜ê°• ì´ë ¥ ë¶€ë¶„ ë¶„ë¦¬
        basic_info = lines[:course_history_start+1]  # "ìˆ˜ê°• ì´ë ¥:" ê¹Œì§€ í¬í•¨
        course_lines = lines[course_history_start+1:]

        # 4. ê° ê°•ì˜ ë¸”ë¡ íŒŒì‹± (ê°•ì˜ëª…ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë¸”ë¡ë“¤)
        courses = []
        current_course = []

        for line in course_lines:
            if line.startswith('ê°•ì˜ëª…:'):
                if current_course:  # ì´ì „ ê°•ì˜ ì €ì¥
                    courses.append('\n'.join(current_course))
                current_course = [line]
            else:
                current_course.append(line)

        if current_course:  # ë§ˆì§€ë§‰ ê°•ì˜ ì €ì¥
            courses.append('\n'.join(current_course))

        # 5. ë’¤ì—ì„œë¶€í„° ê³¼ëª©ì„ í•˜ë‚˜ì”© ì œê±°í•˜ë©´ì„œ í† í° ìˆ˜ ì²´í¬
        while courses and len(courses) > 5:  # ìµœì†Œ 5ê°œëŠ” ìœ ì§€
            # í˜„ì¬ ìƒíƒœë¡œ í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
            trimmed_text = '\n'.join(basic_info + ['\n'.join(courses)])

            if self.count_tokens(trimmed_text) <= max_tokens:
                return trimmed_text

            # ë§ˆì§€ë§‰ ê³¼ëª© ì œê±°
            courses.pop()

        # 6. ìµœì¢… í…ìŠ¤íŠ¸ ë°˜í™˜ (5ê°œ ì´í•˜ê°€ ë˜ì–´ë„ í† í°ì´ ì´ˆê³¼í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜)
        final_text = '\n'.join(basic_info + ['\n'.join(courses)])
        return final_text

    def _evaluate_retrieval(self, test_queries: List[Dict[str, Any]], components: Dict[str, Any]) -> List[QueryResult]:
        """ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ ìˆ˜í–‰"""
        embedder = components['embedder']
        retriever = components['retriever']
        evaluator = components['evaluator']

        query_results = []
        skipped_queries = 0
        TOKEN_LIMIT = 8000  # ì•ˆì „ ë§ˆì§„ í¬í•¨

        for i, query_data in enumerate(test_queries):
            # ì²« ë²ˆì§¸ ì¿¼ë¦¬ ë°ì´í„° êµ¬ì¡° í™•ì¸ (ë””ë²„ê¹…ìš©)
            if i == 0:
                print(f"ì²« ë²ˆì§¸ ì¿¼ë¦¬ ë°ì´í„° íƒ€ì…: {type(query_data)}")
                print(f"ì²« ë²ˆì§¸ ì¿¼ë¦¬ ë‚´ìš©: {str(query_data)[:200]}...")

            # íƒ€ì… ì²´í¬ ë° íŒŒì‹±
            if isinstance(query_data, str):
                try:
                    import json
                    query_data = json.loads(query_data)
                except json.JSONDecodeError as e:
                    print(f"JSON íŒŒì‹± ì‹¤íŒ¨, ì¿¼ë¦¬ ìŠ¤í‚µ: {e}")
                    skipped_queries += 1
                    continue

            # ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ìŠ¤í‚µ
            if not isinstance(query_data, dict):
                print(f"ì˜ëª»ëœ ë°ì´í„° íƒ€ì…, ì¿¼ë¦¬ ìŠ¤í‚µ: {type(query_data)}")
                skipped_queries += 1
                continue

            # í•„ìˆ˜ í•„ë“œ ì²´í¬
            if 'query' not in query_data:
                print(f"'query' í•„ë“œ ì—†ìŒ, ì¿¼ë¦¬ ìŠ¤í‚µ")
                skipped_queries += 1
                continue

            query_text = query_data['query']
            ground_truth = query_data.get('ground_truth_docs', [])

            # í† í° ìˆ˜ ì²´í¬ ë° í•„ìš”ì‹œ ìˆ˜ê°• ì´ë ¥ íŠ¸ë¦¬ë°
            original_token_count = self.count_tokens(query_text)
            if original_token_count > TOKEN_LIMIT:
                print(f"í† í° ì´ˆê³¼ ê°ì§€ ({original_token_count}), ìˆ˜ê°• ì´ë ¥ íŠ¸ë¦¬ë° ì‹œë„...")
                query_text = self.trim_courses_if_needed(query_text, TOKEN_LIMIT)
                new_token_count = self.count_tokens(query_text)

                if new_token_count > TOKEN_LIMIT:
                    print(f"ì¿¼ë¦¬ ìŠ¤í‚µ (íŠ¸ë¦¬ë° í›„ì—ë„ í† í° ì´ˆê³¼: {new_token_count}): {query_text[:50]}...")
                    skipped_queries += 1
                    continue
                else:
                    print(f"íŠ¸ë¦¬ë° ì„±ê³µ: {original_token_count} â†’ {new_token_count} í† í°")

            try:
                # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
                query_embedding = embedder.embed([query_text])[0]

                # ê²€ìƒ‰ ìˆ˜í–‰
                search_results = retriever.search(
                    query_embedding,
                    top_k=self.config.retriever.top_k
                )

                # ê²€ìƒ‰ ê²°ê³¼ ë””ë²„ê¹… (ì²« ë²ˆì§¸ ì¿¼ë¦¬ë§Œ)
                if len(query_results) == 0:
                    print(f"ê²€ìƒ‰ ê²°ê³¼ êµ¬ì¡° ë””ë²„ê¹…:")
                    print(f"  search_results íƒ€ì…: {type(search_results)}")
                    print(f"  search_results ê¸¸ì´: {len(search_results)}")
                    if len(search_results) > 0:
                        print(f"  ì²« ë²ˆì§¸ ê²°ê³¼ íƒ€ì…: {type(search_results[0])}")
                        print(f"  ì²« ë²ˆì§¸ ê²°ê³¼ ë‚´ìš©: {str(search_results[0])[:200]}...")
                        if isinstance(search_results[0], tuple):
                            doc, score = search_results[0]
                            print(f"  doc íƒ€ì…: {type(doc)}")
                            print(f"  doc ë‚´ìš©: {str(doc)[:200]}...")
                            print(f"  score íƒ€ì…: {type(score)}")
                            print(f"  score ê°’: {score}")

                # QueryResult ê°ì²´ ìƒì„±
                try:
                    retrieved_docs = []
                    for item in search_results:
                        if isinstance(item, tuple) and len(item) == 2:
                            doc, score = item
                            if isinstance(doc, dict):
                                retrieved_docs.append({"text": doc.get("text", ""), "metadata": doc.get("metadata", {})})
                            else:
                                print(f"ì˜ˆìƒê³¼ ë‹¤ë¥¸ doc íƒ€ì…: {type(doc)}, ë‚´ìš©: {doc}")
                        else:
                            print(f"ì˜ˆìƒê³¼ ë‹¤ë¥¸ item êµ¬ì¡°: {type(item)}, ë‚´ìš©: {item}")

                    query_result = QueryResult(
                        query=query_text,
                        retrieved_docs=retrieved_docs,
                        ground_truth_docs=ground_truth
                    )
                except Exception as e:
                    print(f"QueryResult ìƒì„± ì‹¤íŒ¨: {e}")
                    print(f"search_results: {search_results}")
                    continue

                query_results.append(query_result)

                if (i + 1) % 10 == 0:
                    print(f"ì¿¼ë¦¬ í‰ê°€ ì™„ë£Œ: {len(query_results)}/{len(test_queries)}")

            except Exception as e:
                print(f"ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                print(f"ì¿¼ë¦¬ ì¸ë±ìŠ¤: {i}")
                print(f"ì¿¼ë¦¬ í…ìŠ¤íŠ¸: {query_text[:100]}...")
                import traceback
                traceback.print_exc()
                skipped_queries += 1
                continue

        print(f"\nì²˜ë¦¬ ì™„ë£Œ: {len(query_results)}ê°œ, ìŠ¤í‚µ: {skipped_queries}ê°œ")

        # í‰ê°€ ì§€í‘œ ê³„ì‚°
        evaluation_results = evaluator.evaluate(query_results)

        print("\n=== í‰ê°€ ê²°ê³¼ ===")
        for result in evaluation_results:
            print(f"{result.metric_name}: {result.score:.4f}")

        return query_results

    def _save_results(self, query_results: List[QueryResult], components: Dict[str, Any], start_time: float) -> Dict[str, Any]:
        """ì‹¤í—˜ ê²°ê³¼ ì €ì¥"""
        # í‰ê°€ ê²°ê³¼ ê³„ì‚°
        evaluator = components['evaluator']
        evaluation_results = evaluator.evaluate(query_results)

        # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
        results = {
            "experiment_info": {
                "name": self.config.experiment_name,
                "description": self.config.description,
                "experiment_id": self.experiment_id,
                "timestamp": datetime.now().isoformat(),
                "duration_seconds": time.time() - start_time
            },
            "config": {
                "embedder": self.config.embedder.__dict__,
                "chunker": self.config.chunker.__dict__,
                "retriever": self.config.retriever.__dict__,
                "evaluation": self.config.evaluation.__dict__
            },
            "component_info": {
                name: comp.get_model_info() if hasattr(comp, 'get_model_info')
                      else comp.get_chunker_info() if hasattr(comp, 'get_chunker_info')
                      else comp.get_retriever_info() if hasattr(comp, 'get_retriever_info')
                      else {}
                for name, comp in components.items() if hasattr(comp, '__dict__')
            },
            "evaluation_results": [
                {
                    "metric": result.metric_name,
                    "score": result.score,
                    "details": result.details
                }
                for result in evaluation_results
            ],
            "query_count": len(query_results),
            "document_count": components['retriever'].get_document_count()
        }

        # ê²°ê³¼ íŒŒì¼ ì €ì¥
        results_file = self.output_dir / f"results_{self.experiment_id}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        # ìƒì„¸ ì¿¼ë¦¬ ê²°ê³¼ ì €ì¥ (ì˜µì…˜)
        detailed_results_file = self.output_dir / f"detailed_results_{self.experiment_id}.jsonl"
        with open(detailed_results_file, 'w', encoding='utf-8') as f:
            for qr in query_results:
                query_detail = {
                    "query": qr.query,
                    "ground_truth_count": len(qr.ground_truth_docs),
                    "retrieved_count": len(qr.retrieved_docs),
                    "retrieved_doc_ids": [
                        doc.get('metadata', {}).get('rec_idx', 'unknown')
                        for doc in qr.retrieved_docs
                    ]
                }
                f.write(json.dumps(query_detail, ensure_ascii=False) + '\n')

        print(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"  - ìš”ì•½ ê²°ê³¼: {results_file}")
        print(f"  - ìƒì„¸ ê²°ê³¼: {detailed_results_file}")

        return results