"""
Career-HY RAG ì‹¤í—˜ íŒŒì´í”„ë¼ì¸ ë©”ì¸ ë¡œì§
"""

import json
import time
import asyncio
from datetime import datetime
from pathlib import Path
from dataclasses import asdict
from typing import List, Dict, Any, Optional

from .config import ExperimentConfig
from .interfaces.evaluator import QueryResult
from utils.factory import ComponentFactory
from utils.data_loader import S3DataLoader
from utils.embedding_cache import embedding_cache
from utils.sampler import StratifiedSampler, generate_reproducible_seed, analyze_sample_distribution
from implementations.evaluators import SearchMetricsEvaluator
from implementations.evaluators.langsmith_evaluator import CareerHYLangSmithEvaluator


class ExperimentPipeline:
    """RAG ì‹¤í—˜ì„ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ íŒŒì´í”„ë¼ì¸"""

    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.results = {}

        # ì‹¤í—˜ ID ë° ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.experiment_id = config.get_experiment_id()
        self.output_dir = config.get_output_path()

        print(f"ì‹¤í—˜ ì‹œì‘: {config.experiment_name}")
        print(f"ì‹¤í—˜ ID: {self.experiment_id}")
        print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")

    async def run(self) -> Dict[str, Any]:
        """
        ì „ì²´ ì‹¤í—˜ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Returns:
            ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()

        try:
            # 1. ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
            print("\n=== 1. ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ===")
            components = self._initialize_components()

            # 2. ë°ì´í„° ë¡œë“œ
            print("\n=== 2. ë°ì´í„° ë¡œë“œ ===")
            # S3ì˜ ëª¨ë“  ë°ì´í„° ì‚¬ìš©
            documents = self._load_documents()

            # 3. ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ë² ë”©
            print("\n=== 3. ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ë² ë”© ===")
            processed_docs, embeddings = self._process_documents(documents, components)

            # 4. ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶•
            print("\n=== 4. ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶• ===")
            self._build_retrieval_system(processed_docs, embeddings, components)

            # 5. Ground Truth ì¿¼ë¦¬ ë¡œë“œ
            print("\n=== 5. Ground Truth ì¿¼ë¦¬ ë¡œë“œ ===")
            test_queries = self._load_test_queries()

            # 6. ì´ì¤‘ í‰ê°€ ì‹¤í–‰
            print("\n=== 6. ì´ì¤‘ í‰ê°€ ì‹œìŠ¤í…œ ===")
            dual_results = await self._run_dual_evaluation(test_queries, components)

            # 7. ê²°ê³¼ ì €ì¥
            print("\n=== 7. ê²°ê³¼ ì €ì¥ ===")
            results = await self._save_dual_results(dual_results, components, start_time)

            print(f"\nì‹¤í—˜ ì™„ë£Œ! ì´ ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
            return results

        except Exception as e:
            print(f"\nì‹¤í—˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    def _initialize_components(self) -> Dict[str, Any]:
        """ì„¤ì •ì— ë”°ë¼ ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”"""
        components = {}

        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        print(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”: {self.config.embedder.type} - {self.config.embedder.model_name}")
        components['embedder'] = ComponentFactory.create_embedder(self.config.embedder)

        # ì²­í‚¹ ì „ëµ ì´ˆê¸°í™”
        print(f"ì²­í‚¹ ì „ëµ ì´ˆê¸°í™”: {self.config.chunker.type}")
        components['chunker'] = ComponentFactory.create_chunker(self.config.chunker)

        # ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print(f"ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”: {self.config.retriever.type}")
        components['retriever'] = ComponentFactory.create_retriever(self.config.retriever)

        # LLM ëª¨ë¸ ì´ˆê¸°í™” (í•„ìš”í•œ ê²½ìš°)
        if hasattr(self.config, 'llm') and self.config.llm:
            print(f"LLM ëª¨ë¸ ì´ˆê¸°í™”: {self.config.llm.type} - {self.config.llm.model_name}")
            components['llm'] = ComponentFactory.create_llm(self.config.llm)

        # ì‘ë‹µ ìƒì„±ê¸° ì´ˆê¸°í™” (ì„ íƒì )
        if hasattr(self.config, 'response_generator') and self.config.response_generator:
            print(f"ì‘ë‹µ ìƒì„±ê¸° ì´ˆê¸°í™”: {self.config.response_generator.type}")
            components['response_generator'] = ComponentFactory.create_response_generator(self.config.response_generator)

        # í‰ê°€ê¸° ì´ˆê¸°í™”
        components['evaluator'] = SearchMetricsEvaluator(
            k_values=self.config.evaluation.retrieval.k_values
        )

        return components

    def _load_documents(self) -> List[Dict[str, Any]]:
        """S3ì—ì„œ ëª¨ë“  ë¬¸ì„œ ë°ì´í„° ë¡œë“œ"""
        data_loader = S3DataLoader(bucket_name=self.config.data.s3_bucket)

        documents = data_loader.load_documents(
            pdf_prefix=self.config.data.pdf_prefix,
            json_prefix=self.config.data.json_prefix
        )

        print(f"ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
        return documents

    def _process_documents(self, documents: List[Dict[str, Any]], components: Dict[str, Any]) -> tuple:
        """ë¬¸ì„œ ì²­í‚¹ ë° ì„ë² ë”© ì²˜ë¦¬ (ìºì‹± ì§€ì›)"""
        chunker = components['chunker']
        embedder = components['embedder']

        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = embedding_cache.generate_cache_key(
            self.config.embedder,
            self.config.chunker
        )

        # ìºì‹œ í™•ì¸
        if embedding_cache.exists(cache_key):
            print(f"âœ… ê¸°ì¡´ ì„ë² ë”© ìºì‹œ ì‚¬ìš©: {cache_key}")
            cached_documents, cached_embeddings = embedding_cache.load(cache_key)
            return cached_documents, cached_embeddings

        print(f"ğŸ”„ ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„±: {cache_key}")

        all_chunks = []
        all_texts = []

        print("ë¬¸ì„œ ì²­í‚¹ ì¤‘...")
        for i, doc in enumerate(documents):
            # ì²­í‚¹ ìˆ˜í–‰
            chunks = chunker.chunk(doc['text'], doc['metadata'])
            all_chunks.extend(chunks)

            # ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for chunk in chunks:
                all_texts.append(chunk['text'])

            if (i + 1) % 50 == 0:
                print(f"ì²­í‚¹ ì™„ë£Œ: {i + 1}/{len(documents)} ë¬¸ì„œ")

        print(f"ì´ ì²­í¬ ìˆ˜: {len(all_chunks)}")

        # ì„ë² ë”© ìƒì„±
        print("ì„ë² ë”© ìƒì„± ì¤‘...")
        embeddings = embedder.embed(all_texts)

        print(f"ì„ë² ë”© ì™„ë£Œ: {len(embeddings)}ê°œ ë²¡í„°")

        # ìºì‹œì— ì €ì¥
        additional_info = {
            "original_document_count": len(documents),
            "embedder_config": self.config.embedder.__dict__,
            "chunker_config": self.config.chunker.__dict__
        }

        try:
            embedding_cache.save(cache_key, all_chunks, embeddings, additional_info)
        except Exception as e:
            print(f"âš ï¸  ìºì‹œ ì €ì¥ ì‹¤íŒ¨ (ì‹¤í—˜ì€ ê³„ì† ì§„í–‰): {e}")

        return all_chunks, embeddings

    def _build_retrieval_system(self, documents: List[Dict[str, Any]], embeddings: List[List[float]], components: Dict[str, Any]) -> None:
        """ê²€ìƒ‰ ì‹œìŠ¤í…œì— ë¬¸ì„œì™€ ì„ë² ë”© ì¶”ê°€"""
        retriever = components['retriever']

        print("ê²€ìƒ‰ ì‹œìŠ¤í…œì— ë¬¸ì„œ ì¶”ê°€ ì¤‘...")

        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (ì‹¤í—˜ìš©)
        if hasattr(retriever, 'clear_collection'):
            retriever.clear_collection()

        # ë¬¸ì„œì™€ ì„ë² ë”©ì„ ê²€ìƒ‰ ì‹œìŠ¤í…œì— ì¶”ê°€
        retriever.add_documents(documents, embeddings)

        doc_count = retriever.get_document_count()
        print(f"ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ: {doc_count}ê°œ ë¬¸ì„œ")

    def _load_test_queries(self) -> List[Dict[str, Any]]:
        """Ground Truth í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ë¡œë“œ"""
        test_queries_path = self.config.data.test_queries_path

        if not Path(test_queries_path).exists():
            print(f"âš ï¸  í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {test_queries_path}")
            print("ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            return self._create_sample_queries()

        queries = []
        with open(test_queries_path, 'r', encoding='utf-8') as f:
            for line in f:
                query_data = json.loads(line.strip())
                queries.append(query_data)

        print(f"í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ë¡œë“œ ì™„ë£Œ: {len(queries)}ê°œ")
        return queries

    def _create_sample_queries(self) -> List[Dict[str, Any]]:
        """ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„± (Ground Truthê°€ ì—†ì„ ë•Œ)"""
        sample_queries = [
            {
                "query": "ì»´í“¨í„°ê³µí•™ ì „ê³µ ì‹ ì… ê°œë°œì ì±„ìš©ê³µê³ ",
                "ground_truth_docs": [],  # ì‹¤ì œë¡œëŠ” ê´€ë ¨ ë¬¸ì„œ IDë“¤ì´ ë“¤ì–´ê°€ì•¼ í•¨
                "user_profile": {
                    "major": "ì»´í“¨í„°ê³µí•™ê³¼",
                    "interest_job": ["ê°œë°œì", "í”„ë¡œê·¸ë˜ë¨¸"]
                }
            },
            {
                "query": "ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ê´€ë ¨ ì§ë¬´",
                "ground_truth_docs": [],
                "user_profile": {
                    "major": "ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤í•™ê³¼",
                    "interest_job": ["ë°ì´í„° ë¶„ì„ê°€", "ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸"]
                }
            }
        ]

        print(f"ìƒ˜í”Œ ì¿¼ë¦¬ ìƒì„±: {len(sample_queries)}ê°œ")
        return sample_queries

    def count_tokens(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚° (tiktoken ì‚¬ìš©)"""
        try:
            import tiktoken
            encoding = tiktoken.encoding_for_model("text-embedding-ada-002")
            return len(encoding.encode(text))
        except Exception as e:
            print(f"í† í° ì¹´ìš´íŒ… ì‹¤íŒ¨: {e}")
            # ëŒ€ëµì ì¸ ì¶”ì • (1 í† í° â‰ˆ 4 ê¸€ì)
            return len(text) // 4

    def trim_courses_if_needed(self, query_text: str, max_tokens: int = 7500) -> str:
        """í† í° ì´ˆê³¼ ì‹œ ìˆ˜ê°• ì´ë ¥ì—ì„œ ê³¼ëª©ì„ ìˆœì°¨ì ìœ¼ë¡œ ì œê±°"""

        # 1. í˜„ì¬ í† í° ìˆ˜ ì²´í¬
        if self.count_tokens(query_text) <= max_tokens:
            return query_text

        # 2. ìˆ˜ê°• ì´ë ¥ ë¶€ë¶„ ë¶„ë¦¬
        lines = query_text.split('\n')
        course_history_start = -1

        for i, line in enumerate(lines):
            if line.startswith('ìˆ˜ê°• ì´ë ¥:'):
                course_history_start = i
                break

        if course_history_start == -1:
            return query_text  # ìˆ˜ê°• ì´ë ¥ì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

        # 3. ê¸°ë³¸ ì •ë³´ ë¶€ë¶„ê³¼ ìˆ˜ê°• ì´ë ¥ ë¶€ë¶„ ë¶„ë¦¬
        basic_info = lines[:course_history_start+1]  # "ìˆ˜ê°• ì´ë ¥:" ê¹Œì§€ í¬í•¨
        course_lines = lines[course_history_start+1:]

        # 4. ê° ê°•ì˜ ë¸”ë¡ íŒŒì‹± (ê°•ì˜ëª…ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë¸”ë¡ë“¤)
        courses = []
        current_course = []

        for line in course_lines:
            if line.startswith('ê°•ì˜ëª…:'):
                if current_course:  # ì´ì „ ê°•ì˜ ì €ì¥
                    courses.append('\n'.join(current_course))
                current_course = [line]
            else:
                current_course.append(line)

        if current_course:  # ë§ˆì§€ë§‰ ê°•ì˜ ì €ì¥
            courses.append('\n'.join(current_course))

        # 5. ë’¤ì—ì„œë¶€í„° ê³¼ëª©ì„ í•˜ë‚˜ì”© ì œê±°í•˜ë©´ì„œ í† í° ìˆ˜ ì²´í¬
        while courses and len(courses) > 5:  # ìµœì†Œ 5ê°œëŠ” ìœ ì§€
            # í˜„ì¬ ìƒíƒœë¡œ í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
            trimmed_text = '\n'.join(basic_info + ['\n'.join(courses)])

            if self.count_tokens(trimmed_text) <= max_tokens:
                return trimmed_text

            # ë§ˆì§€ë§‰ ê³¼ëª© ì œê±°
            courses.pop()

        # 6. ìµœì¢… í…ìŠ¤íŠ¸ ë°˜í™˜ (5ê°œ ì´í•˜ê°€ ë˜ì–´ë„ í† í°ì´ ì´ˆê³¼í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜)
        final_text = '\n'.join(basic_info + ['\n'.join(courses)])
        return final_text

    async def _generate_response_for_query(
        self,
        query_data: Dict[str, Any],
        query_text: str,
        retrieved_docs: List[Dict[str, Any]],
        response_generator
    ) -> Optional[Dict[str, Any]]:
        """ê°œë³„ ì¿¼ë¦¬ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        try:
            # ì‚¬ìš©ì í”„ë¡œí•„ ì¶”ì¶œ
            user_profile = query_data.get('user_profile', {})

            # ëŒ€í™” ì´ë ¥ ì¶”ì¶œ (ìˆë‹¤ë©´)
            chat_history = query_data.get('chat_history', [])

            # ì‘ë‹µ ìƒì„±
            generated_response = await response_generator.generate(
                query=query_text,
                retrieved_docs=retrieved_docs,
                user_profile=user_profile,
                chat_history=chat_history
            )

            return {
                "content": generated_response.content,
                "recommended_jobs": [job.dict() for job in generated_response.recommended_jobs]
            }

        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    async def _evaluate_retrieval(self, test_queries: List[Dict[str, Any]], components: Dict[str, Any]) -> List[QueryResult]:
        """ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ ìˆ˜í–‰"""
        embedder = components['embedder']
        retriever = components['retriever']
        evaluator = components['evaluator']

        query_results = []
        skipped_queries = 0
        TOKEN_LIMIT = 8000  # ì•ˆì „ ë§ˆì§„ í¬í•¨

        for i, query_data in enumerate(test_queries):
            # ì²« ë²ˆì§¸ ì¿¼ë¦¬ ë°ì´í„° êµ¬ì¡° í™•ì¸ (ë””ë²„ê¹…ìš©)
            if i == 0:
                print(f"ì²« ë²ˆì§¸ ì¿¼ë¦¬ ë°ì´í„° íƒ€ì…: {type(query_data)}")
                print(f"ì²« ë²ˆì§¸ ì¿¼ë¦¬ ë‚´ìš©: {str(query_data)[:200]}...")

            # íƒ€ì… ì²´í¬ ë° íŒŒì‹±
            if isinstance(query_data, str):
                try:
                    import json
                    query_data = json.loads(query_data)
                except json.JSONDecodeError as e:
                    print(f"JSON íŒŒì‹± ì‹¤íŒ¨, ì¿¼ë¦¬ ìŠ¤í‚µ: {e}")
                    skipped_queries += 1
                    continue

            # ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ìŠ¤í‚µ
            if not isinstance(query_data, dict):
                print(f"ì˜ëª»ëœ ë°ì´í„° íƒ€ì…, ì¿¼ë¦¬ ìŠ¤í‚µ: {type(query_data)}")
                skipped_queries += 1
                continue

            # í•„ìˆ˜ í•„ë“œ ì²´í¬
            if 'query' not in query_data:
                print(f"'query' í•„ë“œ ì—†ìŒ, ì¿¼ë¦¬ ìŠ¤í‚µ")
                skipped_queries += 1
                continue

            query_text = query_data['query']
            ground_truth = query_data.get('ground_truth_docs', [])

            # í† í° ìˆ˜ ì²´í¬ ë° í•„ìš”ì‹œ ìˆ˜ê°• ì´ë ¥ íŠ¸ë¦¬ë°
            original_token_count = self.count_tokens(query_text)
            if original_token_count > TOKEN_LIMIT:
                print(f"í† í° ì´ˆê³¼ ê°ì§€ ({original_token_count}), ìˆ˜ê°• ì´ë ¥ íŠ¸ë¦¬ë° ì‹œë„...")
                query_text = self.trim_courses_if_needed(query_text, TOKEN_LIMIT)
                new_token_count = self.count_tokens(query_text)

                if new_token_count > TOKEN_LIMIT:
                    print(f"ì¿¼ë¦¬ ìŠ¤í‚µ (íŠ¸ë¦¬ë° í›„ì—ë„ í† í° ì´ˆê³¼: {new_token_count}): {query_text[:50]}...")
                    skipped_queries += 1
                    continue
                else:
                    print(f"íŠ¸ë¦¬ë° ì„±ê³µ: {original_token_count} â†’ {new_token_count} í† í°")

            try:
                # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
                query_embedding = embedder.embed([query_text])[0]

                # ê²€ìƒ‰ ìˆ˜í–‰
                search_results = retriever.search(
                    query_embedding,
                    top_k=self.config.retriever.top_k
                )

                # ê²€ìƒ‰ ê²°ê³¼ ë””ë²„ê¹… (ì²« ë²ˆì§¸ ì¿¼ë¦¬ë§Œ)
                if len(query_results) == 0:
                    print(f"ê²€ìƒ‰ ê²°ê³¼ êµ¬ì¡° ë””ë²„ê¹…:")
                    print(f"  search_results íƒ€ì…: {type(search_results)}")
                    print(f"  search_results ê¸¸ì´: {len(search_results)}")
                    if len(search_results) > 0:
                        print(f"  ì²« ë²ˆì§¸ ê²°ê³¼ íƒ€ì…: {type(search_results[0])}")
                        print(f"  ì²« ë²ˆì§¸ ê²°ê³¼ ë‚´ìš©: {str(search_results[0])[:200]}...")
                        if isinstance(search_results[0], tuple):
                            doc, score = search_results[0]
                            print(f"  doc íƒ€ì…: {type(doc)}")
                            print(f"  doc ë‚´ìš©: {str(doc)[:200]}...")
                            print(f"  score íƒ€ì…: {type(score)}")
                            print(f"  score ê°’: {score}")

                # QueryResult ê°ì²´ ìƒì„±
                try:
                    retrieved_docs = []
                    for item in search_results:
                        if isinstance(item, tuple) and len(item) == 2:
                            doc, score = item
                            if isinstance(doc, dict):
                                retrieved_docs.append({"text": doc.get("text", ""), "metadata": doc.get("metadata", {})})
                            else:
                                print(f"ì˜ˆìƒê³¼ ë‹¤ë¥¸ doc íƒ€ì…: {type(doc)}, ë‚´ìš©: {doc}")
                        else:
                            print(f"ì˜ˆìƒê³¼ ë‹¤ë¥¸ item êµ¬ì¡°: {type(item)}, ë‚´ìš©: {item}")

                    # ì‘ë‹µ ìƒì„± (ì„ íƒì )
                    generated_response = None
                    if 'response_generator' in components:
                        generated_response = await self._generate_response_for_query(
                            query_data, query_text, retrieved_docs, components['response_generator']
                        )

                    query_result = QueryResult(
                        query=query_text,
                        retrieved_docs=retrieved_docs,
                        ground_truth_docs=ground_truth
                    )

                    # ìƒì„±ëœ ì‘ë‹µì„ query_resultì— ì¶”ê°€ (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
                    if generated_response:
                        query_result.generated_response = generated_response
                except Exception as e:
                    print(f"QueryResult ìƒì„± ì‹¤íŒ¨: {e}")
                    print(f"search_results: {search_results}")
                    continue

                query_results.append(query_result)

                if (i + 1) % 10 == 0:
                    print(f"ì¿¼ë¦¬ í‰ê°€ ì™„ë£Œ: {len(query_results)}/{len(test_queries)}")

            except Exception as e:
                print(f"ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                print(f"ì¿¼ë¦¬ ì¸ë±ìŠ¤: {i}")
                print(f"ì¿¼ë¦¬ í…ìŠ¤íŠ¸: {query_text[:100]}...")
                import traceback
                traceback.print_exc()
                skipped_queries += 1
                continue

        print(f"\nì²˜ë¦¬ ì™„ë£Œ: {len(query_results)}ê°œ, ìŠ¤í‚µ: {skipped_queries}ê°œ")

        # í‰ê°€ ì§€í‘œ ê³„ì‚°
        evaluation_results = evaluator.evaluate(query_results)

        print("\n=== í‰ê°€ ê²°ê³¼ ===")
        for result in evaluation_results:
            print(f"{result.metric_name}: {result.score:.4f}")

        return query_results

    async def _run_dual_evaluation(self, test_queries: List[Dict[str, Any]], components: Dict[str, Any]) -> Dict[str, Any]:
        """ì´ì¤‘ í‰ê°€ ì‹œìŠ¤í…œ: ì „ì²´ ê²€ìƒ‰ í‰ê°€ + ìƒ˜í”Œ ìƒì„± í‰ê°€"""

        dual_results = {}

        # 1. ì „ì²´ ì¿¼ë¦¬ ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€
        print("ğŸ” 1ë‹¨ê³„: ì „ì²´ ì¿¼ë¦¬ ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€")
        print(f"   ëŒ€ìƒ: {len(test_queries)}ê°œ ì¿¼ë¦¬")

        retrieval_results = await self._evaluate_retrieval_only(test_queries, components)
        dual_results['retrieval_evaluation'] = {
            'query_results': retrieval_results,
            'query_count': len(retrieval_results)
        }

        # 2. ìƒ˜í”Œ ì¿¼ë¦¬ ì„ íƒ (ì‘ë‹µ ìƒì„±ê¸°ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
        if 'response_generator' in components:
            print("\nğŸ¯ 2ë‹¨ê³„: ìƒ˜í”Œ ì¿¼ë¦¬ ì„ íƒ")

            # ìƒ˜í”Œë§ ì„¤ì • í™•ì¸
            evaluation_config = getattr(self.config, 'evaluation')
            generation_config = evaluation_config.generation

            # ê¸°ë³¸ê°’ ì„¤ì •
            sample_size = generation_config.sample_size
            sample_strategy = generation_config.sample_strategy
            sample_seed = generation_config.sample_seed

            # ì‹œë“œ ìƒì„± (ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš°)
            if sample_seed is None:
                config_dict = {
                    'embedder': self.config.embedder.__dict__,
                    'chunker': self.config.chunker.__dict__,
                    'retriever': self.config.retriever.__dict__
                }
                sample_seed = generate_reproducible_seed(config_dict)

            print(f"   ìƒ˜í”Œ í¬ê¸°: {sample_size}")
            print(f"   ìƒ˜í”Œë§ ì „ëµ: {sample_strategy}")
            print(f"   ì‹œë“œ: {sample_seed}")

            # ìƒ˜í”Œë§ ìˆ˜í–‰
            sampler = StratifiedSampler(seed=sample_seed)
            sampled_queries = sampler.sample_queries(
                test_queries,
                sample_size=sample_size,
                strategy=sample_strategy
            )

            # ìƒ˜í”Œë§ ë¶„í¬ ë¶„ì„
            distribution_analysis = analyze_sample_distribution(test_queries, sampled_queries)
            print(f"   ìƒ˜í”Œë§ ë¹„ìœ¨: {distribution_analysis['sampling_ratio']:.2%}")

            # 3. ìƒ˜í”Œ ì¿¼ë¦¬ ê²€ìƒ‰ + ì‘ë‹µ ìƒì„± í‰ê°€
            print("\nğŸ¤– 3ë‹¨ê³„: ìƒ˜í”Œ ì¿¼ë¦¬ ì‘ë‹µ ìƒì„± í‰ê°€")
            print(f"   ëŒ€ìƒ: {len(sampled_queries)}ê°œ ì¿¼ë¦¬")

            generation_results = await self._evaluate_generation_for_samples(sampled_queries, components)

            dual_results['generation_evaluation'] = {
                'sampled_queries': sampled_queries,
                'query_results': generation_results,
                'sample_config': {
                    'sample_size': len(sampled_queries),
                    'sample_strategy': sample_strategy,
                    'sample_seed': sample_seed
                },
                'distribution_analysis': distribution_analysis
            }

        else:
            print("\nâš ï¸  ì‘ë‹µ ìƒì„±ê¸°ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ìƒì„± í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            dual_results['generation_evaluation'] = None

        return dual_results

    async def _evaluate_retrieval_only(self, test_queries: List[Dict[str, Any]], components: Dict[str, Any]) -> List[QueryResult]:
        """ê²€ìƒ‰ ì„±ëŠ¥ë§Œ í‰ê°€ (ì‘ë‹µ ìƒì„± ì—†ìŒ)"""

        embedder = components['embedder']
        retriever = components['retriever']

        query_results = []
        skipped_queries = 0
        TOKEN_LIMIT = 8000

        for i, query_data in enumerate(test_queries):
            try:
                # ê¸°ì¡´ _evaluate_retrievalê³¼ ë™ì¼í•œ ì „ì²˜ë¦¬
                if isinstance(query_data, str):
                    try:
                        import json
                        query_data = json.loads(query_data)
                    except json.JSONDecodeError as e:
                        skipped_queries += 1
                        continue

                if not isinstance(query_data, dict) or 'query' not in query_data:
                    skipped_queries += 1
                    continue

                query_text = query_data['query']
                ground_truth = query_data.get('ground_truth_docs', [])

                # í† í° ìˆ˜ ì²´í¬ ë° íŠ¸ë¦¬ë°
                original_token_count = self.count_tokens(query_text)
                if original_token_count > TOKEN_LIMIT:
                    query_text = self.trim_courses_if_needed(query_text, TOKEN_LIMIT)
                    new_token_count = self.count_tokens(query_text)

                    if new_token_count > TOKEN_LIMIT:
                        skipped_queries += 1
                        continue

                # ê²€ìƒ‰ë§Œ ìˆ˜í–‰ (ì‘ë‹µ ìƒì„± ì—†ìŒ)
                query_embedding = embedder.embed([query_text])[0]
                search_results = retriever.search(query_embedding, top_k=self.config.retriever.top_k)

                # QueryResult ìƒì„±
                retrieved_docs = []
                for item in search_results:
                    if isinstance(item, tuple) and len(item) == 2:
                        doc, score = item
                        if isinstance(doc, dict):
                            retrieved_docs.append({"text": doc.get("text", ""), "metadata": doc.get("metadata", {})})

                query_result = QueryResult(
                    query=query_text,
                    retrieved_docs=retrieved_docs,
                    ground_truth_docs=ground_truth
                )

                query_results.append(query_result)

                if (i + 1) % 50 == 0:
                    print(f"   ê²€ìƒ‰ í‰ê°€ ì§„í–‰ë¥ : {len(query_results)}/{len(test_queries)}")

            except Exception as e:
                print(f"   ì¿¼ë¦¬ {i} ê²€ìƒ‰ í‰ê°€ ì‹¤íŒ¨: {e}")
                skipped_queries += 1
                continue

        print(f"   ê²€ìƒ‰ í‰ê°€ ì™„ë£Œ: {len(query_results)}ê°œ ì„±ê³µ, {skipped_queries}ê°œ ìŠ¤í‚µ")
        return query_results

    async def _evaluate_generation_for_samples(
        self,
        sampled_queries: List[Dict[str, Any]],
        components: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ìƒ˜í”Œ ì¿¼ë¦¬ë“¤ì— ëŒ€í•œ ê²€ìƒ‰ + ì‘ë‹µ ìƒì„± í‰ê°€"""

        embedder = components['embedder']
        retriever = components['retriever']
        response_generator = components['response_generator']

        generation_results = []
        TOKEN_LIMIT = 8000

        for i, query_data in enumerate(sampled_queries):
            try:
                query_text = query_data['query']
                ground_truth = query_data.get('ground_truth_docs', [])

                # í† í° ìˆ˜ ì²´í¬ ë° íŠ¸ë¦¬ë°
                original_token_count = self.count_tokens(query_text)
                if original_token_count > TOKEN_LIMIT:
                    query_text = self.trim_courses_if_needed(query_text, TOKEN_LIMIT)

                # ê²€ìƒ‰ ìˆ˜í–‰
                query_embedding = embedder.embed([query_text])[0]
                search_results = retriever.search(query_embedding, top_k=self.config.retriever.top_k)

                # ê²€ìƒ‰ ê²°ê³¼ ì •ë¦¬
                retrieved_docs = []
                for item in search_results:
                    if isinstance(item, tuple) and len(item) == 2:
                        doc, score = item
                        if isinstance(doc, dict):
                            retrieved_docs.append({"text": doc.get("text", ""), "metadata": doc.get("metadata", {})})

                # ì‘ë‹µ ìƒì„±
                generated_response = await self._generate_response_for_query(
                    query_data, query_text, retrieved_docs, response_generator
                )

                # ê²°ê³¼ ì €ì¥
                result = {
                    'query': query_text,
                    'user_profile': query_data.get('user_profile', {}),
                    'ground_truth_docs': ground_truth,
                    'retrieved_docs': retrieved_docs,
                    'generated_response': generated_response
                }

                generation_results.append(result)

                if (i + 1) % 10 == 0:
                    print(f"   ìƒì„± í‰ê°€ ì§„í–‰ë¥ : {i + 1}/{len(sampled_queries)}")

            except Exception as e:
                print(f"   ìƒ˜í”Œ ì¿¼ë¦¬ {i} ìƒì„± í‰ê°€ ì‹¤íŒ¨: {e}")
                continue

        print(f"   ìƒì„± í‰ê°€ ì™„ë£Œ: {len(generation_results)}ê°œ ì„±ê³µ")
        return generation_results

    async def _run_langsmith_evaluation_if_enabled(
        self,
        generation_query_results: List[Dict[str, Any]],
        langsmith_evaluation_results: List
    ):
        """LangSmith í‰ê°€ ì‹¤í–‰ (ì„¤ì •ëœ ê²½ìš°ë§Œ)"""

        # LangSmith ì„¤ì • í™•ì¸
        langsmith_config = getattr(self.config, 'langsmith', None)
        print(f"ğŸ” LangSmith ì„¤ì • í™•ì¸: {langsmith_config}")

        if not langsmith_config or not langsmith_config.enabled:
            print("\nâš ï¸  LangSmith í‰ê°€ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        # í™˜ê²½ë³€ìˆ˜ í™•ì¸
        import os
        api_key = os.getenv('LANGCHAIN_API_KEY')
        print(f"ğŸ” LANGCHAIN_API_KEY ì¡´ì¬: {bool(api_key)}")
        if not api_key:
            print("\nâš ï¸  LANGCHAIN_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ LangSmith í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        try:
            print("\n=== LangSmith ê³ í’ˆì§ˆ í‰ê°€ ===")

            # LangSmith í‰ê°€ê¸° ì´ˆê¸°í™”
            judge_model = langsmith_config.judge_model
            project_name = langsmith_config.project_name
            print(f"ğŸ” Judge ëª¨ë¸: {judge_model}, í”„ë¡œì íŠ¸: {project_name}")
            print(f"ğŸ” í‰ê°€í•  ì¿¼ë¦¬ ìˆ˜: {len(generation_query_results)}")

            langsmith_evaluator = CareerHYLangSmithEvaluator(
                judge_model=judge_model,
                project_name=project_name
            )
            print("âœ… LangSmith í‰ê°€ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

            # í‰ê°€ ì‹¤í–‰
            print("ğŸš€ LangSmith í‰ê°€ ì‹œì‘...")
            evaluation_results = await langsmith_evaluator.evaluate_batch(
                generation_query_results,
                experiment_name=self.config.experiment_name
            )
            print(f"âœ… LangSmith í‰ê°€ ì™„ë£Œ: {len(evaluation_results)}ê°œ ê²°ê³¼")

            langsmith_evaluation_results.extend(evaluation_results)

        except Exception as e:
            print(f"âŒ LangSmith í‰ê°€ ì‹¤íŒ¨: {e}")
            print("ìë™í™”ëœ í‰ê°€ ê²°ê³¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    async def _save_dual_results(self, dual_results: Dict[str, Any], components: Dict[str, Any], start_time: float) -> Dict[str, Any]:
        """ì´ì¤‘ í‰ê°€ ê²°ê³¼ ì €ì¥"""

        # 1. ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€
        retrieval_evaluation = dual_results['retrieval_evaluation']
        retrieval_query_results = retrieval_evaluation['query_results']

        evaluator = components['evaluator']
        retrieval_evaluation_results = evaluator.evaluate(retrieval_query_results)

        print("\n=== ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ ===")
        for result in retrieval_evaluation_results:
            print(f"{result.metric_name}: {result.score:.4f}")

        # 2. ìƒì„± í’ˆì§ˆ í‰ê°€ (ì‘ë‹µ ìƒì„±ê¸°ê°€ ìˆëŠ” ê²½ìš°)
        langsmith_evaluation_results = []

        if dual_results['generation_evaluation'] is not None:
            generation_evaluation = dual_results['generation_evaluation']
            generation_query_results = generation_evaluation['query_results']

            if generation_query_results:
                print("\n=== LangSmith ì •ì„±í‰ê°€ ì‹¤í–‰ ===")
                # LangSmith í‰ê°€ë§Œ ì‹¤í–‰
                await self._run_langsmith_evaluation_if_enabled(
                    generation_query_results,
                    langsmith_evaluation_results
                )

        # 3. ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
        results = {
            "experiment_info": {
                "name": self.config.experiment_name,
                "description": self.config.description,
                "experiment_id": self.experiment_id,
                "timestamp": datetime.now().isoformat(),
                "duration_seconds": time.time() - start_time,
                "evaluation_type": "dual"  # ì´ì¤‘ í‰ê°€ í‘œì‹œ
            },
            "config": {
                "embedder": asdict(self.config.embedder),
                "chunker": asdict(self.config.chunker),
                "retriever": asdict(self.config.retriever),
                "evaluation": {
                    "retrieval": asdict(self.config.evaluation.retrieval),
                    "generation": asdict(self.config.evaluation.generation)
                },
                "langsmith": asdict(self.config.langsmith) if self.config.langsmith else None
            },
            "component_info": {
                name: comp.get_model_info() if hasattr(comp, 'get_model_info')
                      else comp.get_chunker_info() if hasattr(comp, 'get_chunker_info')
                      else comp.get_retriever_info() if hasattr(comp, 'get_retriever_info')
                      else {}
                for name, comp in components.items() if hasattr(comp, '__dict__')
            },
            "retrieval_evaluation": {
                "query_count": len(retrieval_query_results),
                "metrics": [
                    {
                        "metric": result.metric_name,
                        "score": result.score,
                        "details": result.details
                    }
                    for result in retrieval_evaluation_results
                ]
            },
            "document_count": components['retriever'].get_document_count()
        }

        # ìƒì„± í‰ê°€ ê²°ê³¼ ì¶”ê°€
        if dual_results['generation_evaluation'] is not None:
            generation_evaluation = dual_results['generation_evaluation']

            results["generation_evaluation"] = {
                "sample_count": len(generation_evaluation['query_results']),
                "sample_config": generation_evaluation['sample_config'],
                "distribution_analysis": generation_evaluation['distribution_analysis'],
                "langsmith_metrics": [
                    {
                        "metric": result.metric_name,
                        "score": result.score,
                        "reasoning": result.reasoning,
                        "details": result.details
                    }
                    for result in langsmith_evaluation_results
                ] if langsmith_evaluation_results else []
            }

            # ì‘ë‹µ ìƒì„±ê¸° ì„¤ì • ì¶”ê°€
            if hasattr(self.config, 'response_generator'):
                results["config"]["response_generator"] = asdict(self.config.response_generator)

            # LangSmith ì„¤ì • ì¶”ê°€ (ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬ë˜ì—ˆìŒ)
            # if hasattr(self.config, 'langsmith'):
            #     results["config"]["langsmith"] = asdict(self.config.langsmith)

        # 4. ê²°ê³¼ íŒŒì¼ ì €ì¥
        results_file = self.output_dir / f"results_{self.experiment_id}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        # 5. ìƒì„¸ ê²°ê³¼ ì €ì¥
        # ê²€ìƒ‰ ê²°ê³¼
        retrieval_detailed_file = self.output_dir / f"retrieval_detailed_{self.experiment_id}.jsonl"
        with open(retrieval_detailed_file, 'w', encoding='utf-8') as f:
            for qr in retrieval_query_results:
                query_detail = {
                    "query": qr.query,
                    "ground_truth_count": len(qr.ground_truth_docs),
                    "retrieved_count": len(qr.retrieved_docs),
                    "retrieved_doc_ids": [
                        doc.get('metadata', {}).get('rec_idx', 'unknown')
                        for doc in qr.retrieved_docs
                    ]
                }
                f.write(json.dumps(query_detail, ensure_ascii=False) + '\n')

        # ìƒì„± ê²°ê³¼ (ìˆë‹¤ë©´)
        if dual_results['generation_evaluation'] is not None:
            generation_evaluation = dual_results['generation_evaluation']
            generation_results = generation_evaluation['query_results']

            if generation_results:
                generation_detailed_file = self.output_dir / f"generation_detailed_{self.experiment_id}.jsonl"
                with open(generation_detailed_file, 'w', encoding='utf-8') as f:
                    for result in generation_results:
                        f.write(json.dumps(result, ensure_ascii=False) + '\n')

                # ìƒì„±ëœ ì‘ë‹µë§Œ ë³„ë„ ì €ì¥
                responses_file = self.output_dir / f"generated_responses_{self.experiment_id}.json"
                responses_only = []
                for result in generation_results:
                    if result.get('generated_response'):
                        responses_only.append({
                            "query": result['query'],
                            "response": result['generated_response']
                        })

                with open(responses_file, 'w', encoding='utf-8') as f:
                    json.dump(responses_only, f, ensure_ascii=False, indent=2)

                print(f"  - ìƒì„± ìƒì„¸ ê²°ê³¼: {generation_detailed_file}")
                print(f"  - ìƒì„±ëœ ì‘ë‹µ: {responses_file}")

        print(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"  - ìš”ì•½ ê²°ê³¼: {results_file}")
        print(f"  - ê²€ìƒ‰ ìƒì„¸ ê²°ê³¼: {retrieval_detailed_file}")

        return results