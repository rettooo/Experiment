"""
ì„ë² ë”© ìºì‹± ì‹œìŠ¤í…œ
ì„ë² ë”© ëª¨ë¸ê³¼ ì²­í‚¹ ì „ëµ ì¡°í•©ë³„ë¡œ ì„ë² ë”© ê²°ê³¼ë¥¼ ìºì‹œ
"""

import os
import json
import pickle
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime

from core.config import EmbedderConfig, ChunkerConfig


class EmbeddingCache:
    """ì„ë² ë”© ê²°ê³¼ ìºì‹± ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, cache_dir: str = "cache/embeddings"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def generate_cache_key(self, embedder_config: EmbedderConfig, chunker_config: ChunkerConfig) -> str:
        """
        ì„ë² ë”© ëª¨ë¸ê³¼ ì²­í‚¹ ì „ëµìœ¼ë¡œ ìºì‹œ í‚¤ ìƒì„±

        Args:
            embedder_config: ì„ë² ë”© ëª¨ë¸ ì„¤ì •
            chunker_config: ì²­í‚¹ ì „ëµ ì„¤ì •

        Returns:
            ìºì‹œ í‚¤ ë¬¸ìì—´
        """
        # ì„ë² ë”© ëª¨ë¸ ë¶€ë¶„
        model_part = embedder_config.model_name.replace("-", "_").replace(".", "_")

        # ì²­í‚¹ ì „ëµ ë¶€ë¶„
        if chunker_config.type == "no_chunk":
            chunk_part = "no_chunk"
        else:
            chunk_part = f"{chunker_config.type}"
            if chunker_config.chunk_size is not None:
                chunk_part += f"_{chunker_config.chunk_size}"
            if chunker_config.chunk_overlap is not None:
                chunk_part += f"_{chunker_config.chunk_overlap}"

        cache_key = f"{model_part}_{chunk_part}"
        return cache_key

    def get_cache_path(self, cache_key: str) -> Path:
        """ìºì‹œ í‚¤ì— í•´ë‹¹í•˜ëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ ë°˜í™˜"""
        return self.cache_dir / cache_key

    def exists(self, cache_key: str) -> bool:
        """ìºì‹œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        cache_path = self.get_cache_path(cache_key)
        required_files = [
            "processed_documents.pkl",
            "embeddings.npy",
            "metadata.json"
        ]

        return all((cache_path / file).exists() for file in required_files)

    def save(
        self,
        cache_key: str,
        processed_documents: List[Dict[str, Any]],
        embeddings: List[List[float]],
        additional_info: Dict[str, Any] = None
    ) -> None:
        """
        ì„ë² ë”© ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥

        Args:
            cache_key: ìºì‹œ í‚¤
            processed_documents: ì²­í‚¹ëœ ë¬¸ì„œë“¤
            embeddings: ì„ë² ë”© ë²¡í„°ë“¤
            additional_info: ì¶”ê°€ ì •ë³´
        """
        cache_path = self.get_cache_path(cache_key)
        cache_path.mkdir(parents=True, exist_ok=True)

        try:
            # 1. ì²˜ë¦¬ëœ ë¬¸ì„œë“¤ ì €ì¥
            with open(cache_path / "processed_documents.pkl", 'wb') as f:
                pickle.dump(processed_documents, f)

            # 2. ì„ë² ë”© ë²¡í„°ë“¤ ì €ì¥ (numpy í˜•íƒœë¡œ)
            embeddings_array = np.array(embeddings)
            np.save(cache_path / "embeddings.npy", embeddings_array)

            # 3. ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                "cache_key": cache_key,
                "document_count": len(processed_documents),
                "embedding_dimension": len(embeddings[0]) if embeddings else 0,
                "created_at": datetime.now().isoformat(),
                "additional_info": additional_info or {}
            }

            with open(cache_path / "metadata.json", 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            print(f"âœ… ì„ë² ë”© ìºì‹œ ì €ì¥ ì™„ë£Œ: {cache_key}")
            print(f"   ë¬¸ì„œ ìˆ˜: {len(processed_documents)}")
            print(f"   ì„ë² ë”© ì°¨ì›: {len(embeddings[0]) if embeddings else 0}")
            print(f"   ì €ì¥ ê²½ë¡œ: {cache_path}")

        except Exception as e:
            print(f"âŒ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ ({cache_key}): {e}")
            # ì‹¤íŒ¨ì‹œ ë¶€ë¶„ì ìœ¼ë¡œ ìƒì„±ëœ íŒŒì¼ë“¤ ì •ë¦¬
            self._cleanup_partial_cache(cache_path)
            raise

    def load(self, cache_key: str) -> Tuple[List[Dict[str, Any]], List[List[float]]]:
        """
        ìºì‹œì—ì„œ ì„ë² ë”© ê²°ê³¼ ë¡œë“œ

        Args:
            cache_key: ìºì‹œ í‚¤

        Returns:
            (processed_documents, embeddings) íŠœí”Œ
        """
        if not self.exists(cache_key):
            raise ValueError(f"ìºì‹œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {cache_key}")

        cache_path = self.get_cache_path(cache_key)

        try:
            # 1. ì²˜ë¦¬ëœ ë¬¸ì„œë“¤ ë¡œë“œ
            with open(cache_path / "processed_documents.pkl", 'rb') as f:
                processed_documents = pickle.load(f)

            # 2. ì„ë² ë”© ë²¡í„°ë“¤ ë¡œë“œ
            embeddings_array = np.load(cache_path / "embeddings.npy")
            embeddings = embeddings_array.tolist()

            # 3. ë©”íƒ€ë°ì´í„° ë¡œë“œ (í™•ì¸ìš©)
            with open(cache_path / "metadata.json", 'r', encoding='utf-8') as f:
                metadata = json.load(f)

            print(f"âœ… ì„ë² ë”© ìºì‹œ ë¡œë“œ ì™„ë£Œ: {cache_key}")
            print(f"   ë¬¸ì„œ ìˆ˜: {metadata['document_count']}")
            print(f"   ì„ë² ë”© ì°¨ì›: {metadata['embedding_dimension']}")
            print(f"   ìƒì„±ì¼: {metadata['created_at']}")

            return processed_documents, embeddings

        except Exception as e:
            print(f"âŒ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨ ({cache_key}): {e}")
            raise

    def get_metadata(self, cache_key: str) -> Dict[str, Any]:
        """ìºì‹œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ"""
        if not self.exists(cache_key):
            return None

        cache_path = self.get_cache_path(cache_key)
        try:
            with open(cache_path / "metadata.json", 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return None

    def list_caches(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  ìºì‹œ ëª©ë¡ ì¡°íšŒ"""
        caches = []

        for cache_dir in self.cache_dir.iterdir():
            if cache_dir.is_dir():
                cache_key = cache_dir.name
                metadata = self.get_metadata(cache_key)
                if metadata:
                    caches.append({
                        "cache_key": cache_key,
                        "metadata": metadata,
                        "path": str(cache_dir)
                    })

        return sorted(caches, key=lambda x: x["metadata"]["created_at"], reverse=True)

    def delete_cache(self, cache_key: str) -> bool:
        """íŠ¹ì • ìºì‹œ ì‚­ì œ"""
        cache_path = self.get_cache_path(cache_key)

        if not cache_path.exists():
            print(f"âš ï¸  ìºì‹œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {cache_key}")
            return False

        try:
            import shutil
            shutil.rmtree(cache_path)
            print(f"ğŸ—‘ï¸  ìºì‹œ ì‚­ì œ ì™„ë£Œ: {cache_key}")
            return True
        except Exception as e:
            print(f"âŒ ìºì‹œ ì‚­ì œ ì‹¤íŒ¨ ({cache_key}): {e}")
            return False

    def get_cache_size(self, cache_key: str) -> int:
        """ìºì‹œ í¬ê¸° ì¡°íšŒ (ë°”ì´íŠ¸)"""
        if not self.exists(cache_key):
            return 0

        cache_path = self.get_cache_path(cache_key)
        total_size = 0

        for file in cache_path.iterdir():
            if file.is_file():
                total_size += file.stat().st_size

        return total_size

    def _cleanup_partial_cache(self, cache_path: Path) -> None:
        """ë¶€ë¶„ì ìœ¼ë¡œ ìƒì„±ëœ ìºì‹œ íŒŒì¼ë“¤ ì •ë¦¬"""
        try:
            if cache_path.exists():
                import shutil
                shutil.rmtree(cache_path)
        except Exception:
            pass

    def print_cache_stats(self) -> None:
        """ìºì‹œ í†µê³„ ì¶œë ¥"""
        caches = self.list_caches()

        print("\n" + "="*60)
        print("ì„ë² ë”© ìºì‹œ í˜„í™©")
        print("="*60)

        if not caches:
            print("ìºì‹œëœ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        total_size = 0
        for cache_info in caches:
            cache_key = cache_info["cache_key"]
            metadata = cache_info["metadata"]
            size = self.get_cache_size(cache_key)
            total_size += size

            print(f"\nğŸ“¦ {cache_key}")
            print(f"   ë¬¸ì„œ ìˆ˜: {metadata['document_count']}")
            print(f"   ì„ë² ë”© ì°¨ì›: {metadata['embedding_dimension']}")
            print(f"   í¬ê¸°: {size / 1024 / 1024:.2f} MB")
            print(f"   ìƒì„±ì¼: {metadata['created_at']}")

        print(f"\nì´ ìºì‹œ í¬ê¸°: {total_size / 1024 / 1024:.2f} MB")
        print(f"ì´ ìºì‹œ ìˆ˜: {len(caches)}")


# ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
embedding_cache = EmbeddingCache()